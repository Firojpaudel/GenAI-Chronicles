{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/GenAI-Chronicles/blob/main/GANs/GAN_With_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trying to implement GAN using PyTorch**: MNIST Dataset\n",
        "---"
      ],
      "metadata": {
        "id": "YrVMe8W0PCPo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7ewqc_1REcDT"
      },
      "outputs": [],
      "source": [
        "##@ Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Creating the Generator"
      ],
      "metadata": {
        "id": "3PZ_TThRr3yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@ First lets create the generator nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, latent_dim, img_shape):\n",
        "    super(Generator, self).__init__()\n",
        "    self.img_shape = img_shape\n",
        "\n",
        "    def block(in_features, out_features, normalize= True):\n",
        "      layers = [nn.Linear(in_features, out_features)]\n",
        "      if normalize:\n",
        "        layers.append(nn.BatchNorm1d(out_features, momentum=0.8))\n",
        "      layers.append(nn.LeakyReLU(0.2, inplace= True))\n",
        "      return layers\n",
        "\n",
        "    self.model= nn.Sequqntial(\n",
        "        *block(latent_dim, 128, normalize = False),\n",
        "        *block(128, 256),\n",
        "        *block(256, 512),\n",
        "        *block(512, 1024),\n",
        "        nn.Linear(1024, int(np.prod(img_shape))),  #Trying to match with the dimension of the image and then we apply the activation function\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, z):\n",
        "    img = self.model(z)\n",
        "    return img.view(img.size(0), *self.img_shape)"
      ],
      "metadata": {
        "id": "j4ClbOZpVlnY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Explaining the code above:** _Generator_\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "RNbTwJ1fl2MS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so first lets explain the block function and the need for it:\n",
        "\n",
        "If we did not use block function the code snippet for `self.model` would look like:\n",
        "\n",
        "```python\n",
        " self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),  # First layer\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # Activation\n",
        "            nn.Linear(128, 256),  # Second layer\n",
        "            nn.BatchNorm1d(256, momentum=0.8),  # Batch normalization\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # Activation\n",
        "            nn.Linear(256, 512),  # Third layer\n",
        "            nn.BatchNorm1d(512, momentum=0.8),  # Batch normalization\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # Activation\n",
        "            nn.Linear(512, 1024),  # Fourth layer\n",
        "            nn.BatchNorm1d(1024, momentum=0.8),  # Batch normalization\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # Activation\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),  # Output layer\n",
        "            nn.Tanh()  # Final activation function\n",
        "        )\n",
        "\n",
        "```\n",
        "\n",
        "which is obviously very tough to handle. So we just created the function `\"block\"` which opens up in the model created.\n",
        "\n",
        "> **_Note:_** \\\n",
        "When you call `*block(...)` inside `nn.Sequential`, the asterisk _unpacks the list of layers returned by the block function_. Without the asterisk, the code would pass the whole list as a single element, which would cause an error.\n",
        "\n",
        "The same goes for ` *self.img_shape`. It's just unpacking the image dimensions\n",
        "\n"
      ],
      "metadata": {
        "id": "wzJlV7qpmHMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Creating the discriminator"
      ],
      "metadata": {
        "id": "Cgxvyizurowa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Now Creating the discriminator:\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, img_shape):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.model= nn.Sequential(\n",
        "        nn.Linear(int(np.prod(img_shape)), 512),\n",
        "        nn.LeakyReLU(0.2, inplace= True),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2, inplace= True),\n",
        "        nn.Linear(256, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    def forward(self, img):\n",
        "      img_flat = img.view(img.size(0), -1)\n",
        "      validity = self.model(img_flat)\n",
        "      return validity"
      ],
      "metadata": {
        "id": "y2t53PSjnqsq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Explaining the code above:** _Discriminator_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WTkXfWYPv-Tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, so here, its pretty straight forward. We create the model which has the final activation funciton of `sigmoid` and this simply classifies as real or fake.\n",
        "\n",
        "> 1 being the absolute real image classification and 0 being the absolute fake image classification\n",
        "\n",
        "Also lets discuss about the `img.view`:\n",
        "\n",
        "Explaining with the example case,\n",
        "\n",
        "Let's say we have a batch of **3 RGB images, each of size 64x64**:\n",
        "\n",
        "- `img.shape` would be $(3, 3, 64, 64)$ ie. $(\\text{batch_size}, \\text{channels}, \\text{height}, \\text{width})$.\n",
        "\n",
        "`img.view(img.size(0), -1)` would do the following:\n",
        "- `img.size(0)` is 3 (the batch size).\n",
        "- `-1` calculates $3 * 64 * 64 = 12288$.\n",
        "\n",
        "The resulting shape would be (3, 12288)"
      ],
      "metadata": {
        "id": "yGdMmqi6wFH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Defining the Loss Function"
      ],
      "metadata": {
        "id": "_a8L6AVTwes5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##@ Loss Function\n",
        "\n",
        "adversarial_loss = torch.nn.BCELoss() #The adversarial loss is simply the Binary Cross Entropy Loss"
      ],
      "metadata": {
        "id": "mpGWNNoOyb7y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Initialize the Generator and Disriminator"
      ],
      "metadata": {
        "id": "PNrAoSTHy0HL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4aCVCQeVy477"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7Wcaj3Rcml+mjno6EGFej",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}