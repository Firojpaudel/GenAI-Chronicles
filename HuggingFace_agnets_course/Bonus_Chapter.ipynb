{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Fine Tuning \n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"Model used: \"google/gemma-2-2b-it\"","metadata":{}},{"cell_type":"markdown","source":"#### Installing Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes \n!pip install -q -U peft\n!pip install -q -U trl\n!pip install -q -U tensorboardX\n!pip install -q wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:57:28.112925Z","iopub.execute_input":"2025-03-05T12:57:28.113223Z","iopub.status.idle":"2025-03-05T12:57:48.921865Z","shell.execute_reply.started":"2025-03-05T12:57:28.113200Z","shell.execute_reply":"2025-03-05T12:57:48.920787Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"- `bitsandbytes` for quantization\n- `peft` for LoRA adapters\n- `trl` for the trainer class","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:57:48.923266Z","iopub.execute_input":"2025-03-05T12:57:48.923640Z","iopub.status.idle":"2025-03-05T12:57:49.138591Z","shell.execute_reply.started":"2025-03-05T12:57:48.923598Z","shell.execute_reply":"2025-03-05T12:57:49.137905Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from enum import Enum\nfrom functools import partial\nimport pandas as pd\nimport torch\nimport json\n\nfrom accelerate import Accelerator\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, TrainingArguments\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\nfrom peft import LoraConfig, TaskType\n\nseed = 42\nset_seed(seed)\n\nimport os\n\nos.environ['HF_TOKEN']= secret_value_0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:57:49.140213Z","iopub.execute_input":"2025-03-05T12:57:49.140528Z","iopub.status.idle":"2025-03-05T12:58:13.720245Z","shell.execute_reply.started":"2025-03-05T12:57:49.140491Z","shell.execute_reply":"2025-03-05T12:58:13.719390Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"accelerator = Accelerator(\n    mixed_precision='bf16',\n    gradient_accumulation_steps=4,\n    dataloader_config={\n        \"split_batches\": False,\n        \"dispatch_batches\": True, \n        \"drop_last\": True\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:58:13.721491Z","iopub.execute_input":"2025-03-05T12:58:13.721770Z","iopub.status.idle":"2025-03-05T12:58:13.994155Z","shell.execute_reply.started":"2025-03-05T12:58:13.721740Z","shell.execute_reply":"2025-03-05T12:58:13.993510Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model_name = \"google/gemma-2-2b-it\"\ndataset_name = \"Jofthomas/hermes-function-calling-thinking-V1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n\ndef preprocess(sample):\n      messages = sample[\"messages\"]\n      first_message = messages[0]\n\n      # Instead of adding a system message, we merge the content into the first user message\n      if first_message[\"role\"] == \"system\":\n          system_message_content = first_message[\"content\"]\n          # Merge system content with the first user message\n          messages[1][\"content\"] = system_message_content + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\" + messages[1][\"content\"]\n          # Remove the system message from the conversation\n          messages.pop(0)\n\n      return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n\ndataset = load_dataset(dataset_name)\ndataset = dataset.rename_column(\"conversations\", \"messages\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:58:13.994852Z","iopub.execute_input":"2025-03-05T12:58:13.995083Z","iopub.status.idle":"2025-03-05T12:58:18.954666Z","shell.execute_reply.started":"2025-03-05T12:58:13.995063Z","shell.execute_reply":"2025-03-05T12:58:18.954014Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d915587b0298421091fc9a71d495cf6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f29e662219c64210b981105830032670"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c99b214388f4590bf5b07a5f39cb62b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5acbf9328f724d9e865a4ebef1c5582e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/354 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"159762eb8ca44befbdcd93fda985a244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88490d950db4fd79d8b0d479d23caa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f669b0a3cf64ae887b18e3ff5e31255"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataset = dataset.map(preprocess, remove_columns=\"messages\")\ndataset = dataset[\"train\"].train_test_split(0.1)\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:58:18.955362Z","iopub.execute_input":"2025-03-05T12:58:18.955613Z","iopub.status.idle":"2025-03-05T12:58:20.446782Z","shell.execute_reply.started":"2025-03-05T12:58:18.955581Z","shell.execute_reply":"2025-03-05T12:58:20.446100Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85417f67bca4cb88e051f9aba1f49dd"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 3213\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 357\n    })\n})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(dataset['train']['text'][5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:58:20.447751Z","iopub.execute_input":"2025-03-05T12:58:20.448077Z","iopub.status.idle":"2025-03-05T12:58:20.476530Z","shell.execute_reply.started":"2025-03-05T12:58:20.448043Z","shell.execute_reply":"2025-03-05T12:58:20.475833Z"}},"outputs":[{"name":"stdout","text":"<bos><start_of_turn>human\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'search_movie_theaters', 'description': 'Search for movie theaters near a specific location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The location to search for movie theaters'}}, 'required': ['location']}}}, {'type': 'function', 'function': {'name': 'search_restaurants', 'description': 'Search for restaurants based on cuisine type and location', 'parameters': {'type': 'object', 'properties': {'cuisine': {'type': 'string', 'description': 'The cuisine type to search for'}, 'location': {'type': 'string', 'description': 'The location to search in'}}, 'required': ['cuisine', 'location']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{tool_call}\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\n\nI am in New York. Can you find me some movie theaters near me?<end_of_turn><eos>\n<start_of_turn>model\n<think>Okay, so I need to figure out how to respond to the user's request. Let me start by understanding what the user is asking. They say they're in New York and want me to find some movie theaters near them. Looking at the tools available, there are two functions: search_movie_theaters and search_restaurants.\n\nFirst, I should determine which function is more appropriate for the user's query. Since they're asking about movie theaters, the search_movie_theaters function seems directly relevant. The other function, search_restaurants, is about finding restaurants based on cuisine and location, which doesn't fit here.\n\nNow, looking at the search_movie_theaters function, it requires a 'location' parameter. The user provided their location as New York, so that fits perfectly. I don't need any additional information because the function only asks for the location to search nearby theaters.\n\nI should make sure that I'm not making any assumptions about other parameters. The function doesn't require anything else like a specific movie or theater name, just the location. So, using the location 'New York' should suffice.\n\nPutting it all together, the next step is to call the search_movie_theaters function with the location argument set to 'New York'. This should return the information the user is looking for without any unnecessary steps or parameters.\n</think><tool_call>\n{'name': 'search_movie_theaters', 'arguments': {'location': 'New York'}}\n</tool_call><end_of_turn><eos>\n<start_of_turn>tool\n<tool_response>\n{'status': 'success', 'data': [{'name': 'AMC Empire 25', 'address': '234 W 42nd St, New York, NY 10036'}, {'name': 'Regal Union Square ScreenX & 4DX', 'address': '850 Broadway, New York, NY 10003'}, {'name': 'Cinépolis Chelsea', 'address': '260 W 23rd St, New York, NY 10011'}]}\n</tool_response><end_of_turn><eos>\n<start_of_turn>model\nI found some movie theaters near you in New York. Here are a few options: \n1. AMC Empire 25 located at 234 W 42nd St, New York, NY 10036\n2. Regal Union Square ScreenX & 4DX located at 850 Broadway, New York, NY 10003\n3. Cinépolis Chelsea located at 260 W 23rd St, New York, NY 10011<end_of_turn><eos>\n<start_of_turn>human\nThanks for the information.<end_of_turn><eos>\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(tokenizer.pad_token)\nprint(tokenizer.bos_token)\nprint(tokenizer.eos_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:58:20.478607Z","iopub.execute_input":"2025-03-05T12:58:20.478827Z","iopub.status.idle":"2025-03-05T12:58:20.499825Z","shell.execute_reply.started":"2025-03-05T12:58:20.478808Z","shell.execute_reply":"2025-03-05T12:58:20.498860Z"}},"outputs":[{"name":"stdout","text":"<pad>\n<bos>\n<eos>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"## Modifying the tokenizer\n\nclass ChatmlSpecialTokens(str, Enum):\n    tools = \"<tools>\"\n    eotools = \"</tools>\"\n    think = \"<think>\"\n    eothink = \"</think>\"\n    tool_call=\"<tool_call>\"\n    eotool_call=\"</tool_call>\"\n    tool_response=\"<tool_reponse>\"\n    eotool_response=\"</tool_reponse>\"\n    pad_token = \"<pad>\"\n    eos_token = \"<eos>\"\n    @classmethod\n    def list(cls):\n        return [c.value for c in cls]\n\ntokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        pad_token=ChatmlSpecialTokens.pad_token.value,\n        additional_special_tokens=ChatmlSpecialTokens.list()\n    )\ntokenizer.chat_template = \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             attn_implementation='eager',\n                                             device_map=\"auto\")\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.to(torch.bfloat16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:58:20.500906Z","iopub.execute_input":"2025-03-05T12:58:20.501126Z","iopub.status.idle":"2025-03-05T12:58:59.606212Z","shell.execute_reply.started":"2025-03-05T12:58:20.501106Z","shell.execute_reply":"2025-03-05T12:58:59.605333Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"244690caee3e4350b3e62026983c4478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a26c93f3a40e463292c38c18533342b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77e5440ec19a40b4805ccb7e381e9b6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d22c0282b234ad089db32fde22788f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7a086b23d19463f9e597a2ae700630f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1984265cdc454c14bf7520e6ed8679bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d49467e517f493e84a44db8302112cf"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Gemma2ForCausalLM(\n  (model): Gemma2Model(\n    (embed_tokens): Embedding(256008, 2304, padding_idx=0)\n    (layers): ModuleList(\n      (0-25): 26 x Gemma2DecoderLayer(\n        (self_attn): Gemma2Attention(\n          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n          (rotary_emb): Gemma2RotaryEmbedding()\n        )\n        (mlp): Gemma2MLP(\n          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n      )\n    )\n    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n  )\n  (lm_head): Linear(in_features=2304, out_features=256008, bias=False)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"## Configuring LoRA...\n\nfrom peft import LoraConfig\n\n# TODO: Configure LoRA parameters\n# r: rank dimension for LoRA update matrices (smaller = more compression)\nrank_dimension = 16\n# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\nlora_alpha = 64\n# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\nlora_dropout = 0.05\n\npeft_config = LoraConfig(r=rank_dimension,\n                         lora_alpha=lora_alpha,\n                         lora_dropout=lora_dropout,\n                         target_modules=[\"gate_proj\",\"q_proj\",\"lm_head\",\"o_proj\",\"k_proj\",\"embed_tokens\",\"down_proj\",\"up_proj\",\"v_proj\"], # which layer in the transformers do we target ?\n                         task_type=TaskType.CAUSAL_LM,\n                         layers_to_transform=list(range(0, 28)),  # Only transform first 28 layers\n                         fan_in_fan_out=True  # Avoid device mismatch in matrix ops\n                        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:58:59.607063Z","iopub.execute_input":"2025-03-05T12:58:59.607364Z","iopub.status.idle":"2025-03-05T12:58:59.611805Z","shell.execute_reply.started":"2025-03-05T12:58:59.607335Z","shell.execute_reply":"2025-03-05T12:58:59.611017Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"username=\"Firoj112\"\noutput_dir = \"gemma-2-2B-it-thinking-function_calling-V0\" # The directory where the trained model checkpoints, logs, and other artifacts will be saved. It will also be the default name of the model when pushed to the hub if not redefined later.\nper_device_train_batch_size = 1\nper_device_eval_batch_size = 1\ngradient_accumulation_steps = 4\nlogging_steps = 5\nlearning_rate = 1e-4 # The initial learning rate for the optimizer.\n\nmax_grad_norm = 1.0\nnum_train_epochs=1\nwarmup_ratio = 0.1\nlr_scheduler_type = \"cosine\"\nmax_seq_length = 1500\n\ntraining_arguments = SFTConfig(\n    output_dir=output_dir,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    logging_steps=5,\n    learning_rate=1e-4,\n    max_grad_norm=1.0,\n    num_train_epochs=1,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"tensorboard\",\n    bf16=True,\n    hub_private_repo=False,\n    push_to_hub=False,\n    gradient_checkpointing=True,\n    dataset_text_field=\"text\",\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    packing=True, \n    max_seq_length=1500  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:58:59.612514Z","iopub.execute_input":"2025-03-05T12:58:59.612716Z","iopub.status.idle":"2025-03-05T12:59:01.941110Z","shell.execute_reply.started":"2025-03-05T12:58:59.612698Z","shell.execute_reply":"2025-03-05T12:59:01.940395Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    processing_class=tokenizer,\n    peft_config=peft_config,\n    # Critical for device sync:\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\n# Wrap with accelerator\nmodel, trainer = accelerator.prepare(model, trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:59:01.941844Z","iopub.execute_input":"2025-03-05T12:59:01.942074Z","iopub.status.idle":"2025-03-05T12:59:22.941619Z","shell.execute_reply.started":"2025-03-05T12:59:01.942045Z","shell.execute_reply":"2025-03-05T12:59:22.940690Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py:1255: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/3213 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9874c917552c425fb0cd758e2a91b8f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/3213 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7342263a714e1cb5ab0263d4ffa50d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/3213 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74509426bbe44471b8587991ee612b6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Packing train dataset:   0%|          | 0/3213 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9405856aca64a47979705486f714a8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e352d8fb104ff9980491a2e222477c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"009c1bd1e4b34938bd5161919c07d2b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c931af9cfdf4010a4535648f7e74e7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Packing eval dataset:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd58ab96128e478ea1a7625c156b78ef"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T12:59:22.942573Z","iopub.execute_input":"2025-03-05T12:59:22.942819Z","iopub.status.idle":"2025-03-05T16:07:49.520736Z","shell.execute_reply.started":"2025-03-05T12:59:22.942793Z","shell.execute_reply":"2025-03-05T16:07:49.520001Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='533' max='533' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [533/533 3:08:00, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>9.010800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>8.308400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>7.154300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>6.088100</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>5.254600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>4.245200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>3.534700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.264400</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>2.888000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.740000</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>2.615100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.329800</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>2.073400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.802100</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.692700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.667800</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.643300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.507300</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.643200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.589900</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>1.501200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.625400</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.572200</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.687900</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.572200</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.554500</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>1.499700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.523800</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.523500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.544700</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1.421100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.475800</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>1.343600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.439300</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.405000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.517300</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>1.395700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.446400</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>1.346900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.300800</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>1.440100</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.338800</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>1.229800</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.346200</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.244400</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.181800</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>1.331900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.291800</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>1.204900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.366100</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>1.229100</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.407100</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>1.142100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.411000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.158700</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.166100</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>1.221300</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.209200</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>1.227400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.399500</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>1.356500</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.322600</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>1.231000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.255500</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.161000</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.256000</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>1.328700</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.159500</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>1.274600</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.106500</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>1.120400</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.255600</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>1.440300</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.115200</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.070000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.373500</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>1.335400</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.097000</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>1.223200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.219400</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>1.122100</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.060800</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>1.198100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.181300</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.092400</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.144400</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>1.187500</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.124800</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>1.351100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.086600</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>1.077700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.244600</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>1.353700</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.176400</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.236400</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.423300</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>1.358200</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.161700</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>1.147900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.124700</td>\n    </tr>\n    <tr>\n      <td>505</td>\n      <td>1.086800</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.349200</td>\n    </tr>\n    <tr>\n      <td>515</td>\n      <td>1.155400</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.219700</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.129300</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.125100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"## Pushing the model to the hub \n\ntrainer.push_to_hub(f\"{username}/{output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:07:49.521597Z","iopub.execute_input":"2025-03-05T16:07:49.521900Z","iopub.status.idle":"2025-03-05T16:08:41.739085Z","shell.execute_reply.started":"2025-03-05T16:07:49.521870Z","shell.execute_reply":"2025-03-05T16:08:41.738362Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"198b163ff8e44a3bba91fa82a0ffb341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9de5e7133b294113ada4f8d153e6c808"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6e3ecde4c6c4c389a736cf544c06953"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b55a2caddb704cacb18e6046c421a3e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1741179563.63dfc1162b9a.31.0:   0%|          | 0.00/35.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ab739f278a4202a85f557dfb76efd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90905983ae6141babfbf1adb4384a4ad"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Firoj112/gemma-2-2B-it-thinking-function_calling-V0/commit/e8a7b286b67907c5327bcb34a2bfdc73042c3950', commit_message='Firoj112/gemma-2-2B-it-thinking-function_calling-V0', commit_description='', oid='e8a7b286b67907c5327bcb34a2bfdc73042c3950', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Firoj112/gemma-2-2B-it-thinking-function_calling-V0', endpoint='https://huggingface.co', repo_type='model', repo_id='Firoj112/gemma-2-2B-it-thinking-function_calling-V0'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"tokenizer.eos_token = \"<eos>\"\n# push the tokenizer to hub ( replace with your username and your previously specified\ntokenizer.push_to_hub(f\"{username}/{output_dir}\", token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:08:41.739798Z","iopub.execute_input":"2025-03-05T16:08:41.739999Z","iopub.status.idle":"2025-03-05T16:08:50.213091Z","shell.execute_reply.started":"2025-03-05T16:08:41.739981Z","shell.execute_reply":"2025-03-05T16:08:50.212311Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f9a2ede6eb44cf88044d2c09a511d3"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Firoj112/gemma-2-2B-it-thinking-function_calling-V0/commit/3969e0a0b8ec4728c92358523b0d8de9860dad2f', commit_message='Upload tokenizer', commit_description='', oid='3969e0a0b8ec4728c92358523b0d8de9860dad2f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Firoj112/gemma-2-2B-it-thinking-function_calling-V0', endpoint='https://huggingface.co', repo_type='model', repo_id='Firoj112/gemma-2-2B-it-thinking-function_calling-V0'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":15}]}