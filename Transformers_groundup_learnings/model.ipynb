{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Transformers architecture in detail\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src= \"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"The transformers architecture\" width=\"250px\">\n",
    "    <figcaption><i>The Transformers Architecture from the OG paper</i></figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a foundational understanding of the Transformer architecture from the _'Attention is All You Need'_ paper, but now I'm diving deeper into the concepts. Currently, I'm learning from **Umair Jamil** Sir's 'Coding a Transformers from Scratch' course on PyTorch with **Jay Alamar**'s blog combined.\n",
    "\n",
    "Fingers crossed everything goes smoothly! ðŸ¤ž\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the `input_embedding`Layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src= \"https://images.ctfassets.net/k07f0awoib97/2n4uIQh2bAX7fRmx4AGzyY/a1bc6fa1e2d14ff247716b5f589a2099/Screen_Recording_2023-06-03_at_4.52.54_PM.gif\" alt=\"Input Emedding\" width=\"550px\">\n",
    "    <figcaption><i>The Embedding Mechanism illustrated</i></figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##@ All required imports are here in this cell\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \"The embedding is done in pytorch using just a simple nn.Embedding function\"\n",
    "        self.embed= nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * torch.sqrt(self.d_model, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so the `Embedding` was already done earlier but we scale these embeddings while passing forward, for majorly two reasons:\n",
    "\n",
    "1. The `d_model` scaling is done inorder to ensure the magnitude of input embedding and positional encoding are appropriately balanced.\n",
    "2. Also, it maintains the informational integrity.\n",
    "\n",
    "> _src: \"StackOverflow\"_\n",
    "\n",
    "_Well Mathematically_,\n",
    "\n",
    "Let's assume:\n",
    "\n",
    "- Input embeddings as $\\text{E}$\n",
    "- Positional encodings as $\\text{P}$\n",
    "\n",
    "Then, the combined input to the model would be: $$\\text{X} = \\sqrt{d_{model}} \\times \\text{E} + \\text{P}$$\n",
    "\n",
    "\n",
    "Ofc, it will go through the `softmax` function before that but yeah..\n",
    "\n",
    "This scaling ensures that the variance of the embeddings is in line with the variance of the positional encodings, leading to more stable training and better convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `input_embedding`, time for `positional_encoding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_length: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        ## Create a positional encoding matrix of size seq_length x d_model\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        ## Create a vector of shape (seq_length, 1)\n",
    "        pos = torch.arange(0, seq_length).unsqueeze(1)  ## The numerator part \n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model)) ## The denominator part\n",
    "        \n",
    "        ## Apply sine to even positions\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        \n",
    "        ## Apply cosine to odd positions\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        \n",
    "        ## Add a batch dimension to the positional encoding\\\n",
    "        pe = pe.unsqueeze(0)     \n",
    "        \n",
    "        ## Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explaining the code above:_\n",
    "\n",
    "> \"  Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    " order of the sequence, **we must inject some information about the relative or absolute position of the\n",
    " tokens in the sequence**. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    " bottoms of the encoder and decoder stacks. **The positional encodings have the same dimension dmodel\n",
    " as the embeddings**, so that the two can be summed. There are many choices of positional encodings,\n",
    " learned and fixed \" \n",
    "\n",
    " *-src: paper*\n",
    "\n",
    "Here the authors of this paper have clearly mentioned, first we should inject a sequence and ofc `seq_length` needs to be defined. \n",
    "Next, the dimension we used earlier in the `input_embeddings` is to be used. And, finally there's a `dropout` which is added in order\n",
    "to prevent the model from overfitting.\n",
    "\n",
    "\n",
    "Then again from the same subsection, the `positional_encoding` formulas are provided. We use different formulas for odd and even dimensions.\n",
    "\n",
    "The parameters are same, we just pass through different funcitons. \n",
    "\n",
    "- $sin$ for even dimensions and\n",
    "- $cos$ for odd dimensions.\n",
    "\n",
    "i.e., $$ \\text{PE}_{(pos, 2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$ \n",
    "$$ and, \\text{PE}_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "So, the numerators and denominators are defined before passing through these functions.\n",
    "\n",
    "_Numerator part first:_\n",
    "```python \n",
    "pos = torch.arange(0, seq_len).unsqueeze(1)\n",
    "```\n",
    "\n",
    "Here, the position `pos` is unsquezed to 1, for column vector formatting\n",
    "\n",
    "_Denominator part:_\n",
    "\n",
    "```python\n",
    "div = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)))/ d_model)\n",
    "```\n",
    "\n",
    "Well, it was meant to be $ 10000 ^{\\frac{2i}{d_{model}}}$ but well in coding implementation, we could use log instead.\n",
    "\n",
    "$$ 10000 ^{\\frac{2i}{d_{model}}} \\equiv  exp. (ln(10000) \\cdot \\frac{2i}{d_{model}})$$\n",
    "\n",
    "Also, we are using negative log to ensure the scaling decreases progressively as $i$ increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gen_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
