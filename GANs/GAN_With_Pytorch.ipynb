{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/GenAI-Chronicles/blob/main/GANs/GAN_With_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-7VBFG41oNyE",
        "outputId": "9c5bbcb9-99dc-403c-818f-0990108ecfc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0/200] [Batch 0/938] [D loss: 0.681468] [G loss: 0.691147]\n",
            "[Epoch 0/200] [Batch 100/938] [D loss: 0.741282] [G loss: 0.493891]\n",
            "[Epoch 0/200] [Batch 200/938] [D loss: 0.542678] [G loss: 1.081067]\n",
            "[Epoch 0/200] [Batch 300/938] [D loss: 0.528900] [G loss: 1.483625]\n",
            "[Epoch 0/200] [Batch 400/938] [D loss: 0.652110] [G loss: 0.462227]\n",
            "[Epoch 0/200] [Batch 500/938] [D loss: 0.554159] [G loss: 0.960890]\n",
            "[Epoch 0/200] [Batch 600/938] [D loss: 0.513295] [G loss: 0.887534]\n",
            "[Epoch 0/200] [Batch 700/938] [D loss: 0.556031] [G loss: 0.996662]\n",
            "[Epoch 0/200] [Batch 800/938] [D loss: 0.577245] [G loss: 0.670647]\n",
            "[Epoch 0/200] [Batch 900/938] [D loss: 0.518372] [G loss: 0.900598]\n",
            "[Epoch 1/200] [Batch 0/938] [D loss: 0.462437] [G loss: 1.023750]\n",
            "[Epoch 1/200] [Batch 100/938] [D loss: 0.496170] [G loss: 2.405663]\n",
            "[Epoch 1/200] [Batch 200/938] [D loss: 0.409335] [G loss: 1.717518]\n",
            "[Epoch 1/200] [Batch 300/938] [D loss: 0.441603] [G loss: 0.868198]\n",
            "[Epoch 1/200] [Batch 400/938] [D loss: 0.727007] [G loss: 0.348984]\n",
            "[Epoch 1/200] [Batch 500/938] [D loss: 0.398548] [G loss: 1.166110]\n",
            "[Epoch 1/200] [Batch 600/938] [D loss: 0.561465] [G loss: 0.515062]\n",
            "[Epoch 1/200] [Batch 700/938] [D loss: 0.477576] [G loss: 2.198059]\n",
            "[Epoch 1/200] [Batch 800/938] [D loss: 0.388181] [G loss: 0.972979]\n",
            "[Epoch 1/200] [Batch 900/938] [D loss: 0.456758] [G loss: 1.620907]\n",
            "[Epoch 2/200] [Batch 0/938] [D loss: 0.430886] [G loss: 0.891489]\n",
            "[Epoch 2/200] [Batch 100/938] [D loss: 0.383291] [G loss: 2.114059]\n",
            "[Epoch 2/200] [Batch 200/938] [D loss: 0.515498] [G loss: 1.114621]\n",
            "[Epoch 2/200] [Batch 300/938] [D loss: 0.434333] [G loss: 1.545805]\n",
            "[Epoch 2/200] [Batch 400/938] [D loss: 0.458570] [G loss: 0.822785]\n",
            "[Epoch 2/200] [Batch 500/938] [D loss: 0.802386] [G loss: 0.338778]\n",
            "[Epoch 2/200] [Batch 600/938] [D loss: 0.530254] [G loss: 2.457301]\n",
            "[Epoch 2/200] [Batch 700/938] [D loss: 0.489154] [G loss: 2.118285]\n",
            "[Epoch 2/200] [Batch 800/938] [D loss: 0.368439] [G loss: 1.399247]\n",
            "[Epoch 2/200] [Batch 900/938] [D loss: 0.351894] [G loss: 1.583130]\n",
            "[Epoch 3/200] [Batch 0/938] [D loss: 0.446842] [G loss: 1.117957]\n",
            "[Epoch 3/200] [Batch 100/938] [D loss: 0.517450] [G loss: 0.759437]\n",
            "[Epoch 3/200] [Batch 200/938] [D loss: 0.325214] [G loss: 1.288833]\n",
            "[Epoch 3/200] [Batch 300/938] [D loss: 0.763761] [G loss: 0.369967]\n",
            "[Epoch 3/200] [Batch 400/938] [D loss: 0.353252] [G loss: 1.352609]\n",
            "[Epoch 3/200] [Batch 500/938] [D loss: 0.340398] [G loss: 1.488939]\n",
            "[Epoch 3/200] [Batch 600/938] [D loss: 0.466744] [G loss: 1.359707]\n",
            "[Epoch 3/200] [Batch 700/938] [D loss: 0.439289] [G loss: 1.053036]\n",
            "[Epoch 3/200] [Batch 800/938] [D loss: 0.521307] [G loss: 0.903487]\n",
            "[Epoch 3/200] [Batch 900/938] [D loss: 0.397300] [G loss: 1.030604]\n",
            "[Epoch 4/200] [Batch 0/938] [D loss: 0.358806] [G loss: 1.576545]\n",
            "[Epoch 4/200] [Batch 100/938] [D loss: 0.391957] [G loss: 1.895732]\n",
            "[Epoch 4/200] [Batch 200/938] [D loss: 0.463112] [G loss: 2.511395]\n",
            "[Epoch 4/200] [Batch 300/938] [D loss: 0.943655] [G loss: 0.280497]\n",
            "[Epoch 4/200] [Batch 400/938] [D loss: 0.444934] [G loss: 1.924349]\n",
            "[Epoch 4/200] [Batch 500/938] [D loss: 0.470543] [G loss: 1.077030]\n",
            "[Epoch 4/200] [Batch 600/938] [D loss: 0.388772] [G loss: 1.654480]\n",
            "[Epoch 4/200] [Batch 700/938] [D loss: 0.564606] [G loss: 2.253568]\n",
            "[Epoch 4/200] [Batch 800/938] [D loss: 0.476036] [G loss: 1.650462]\n",
            "[Epoch 4/200] [Batch 900/938] [D loss: 0.714028] [G loss: 0.439105]\n",
            "[Epoch 5/200] [Batch 0/938] [D loss: 0.526688] [G loss: 1.458398]\n",
            "[Epoch 5/200] [Batch 100/938] [D loss: 0.496639] [G loss: 0.891619]\n",
            "[Epoch 5/200] [Batch 200/938] [D loss: 0.646927] [G loss: 0.914559]\n",
            "[Epoch 5/200] [Batch 300/938] [D loss: 0.547911] [G loss: 1.051822]\n",
            "[Epoch 5/200] [Batch 400/938] [D loss: 0.496328] [G loss: 0.873120]\n",
            "[Epoch 5/200] [Batch 500/938] [D loss: 0.546472] [G loss: 0.911528]\n",
            "[Epoch 5/200] [Batch 600/938] [D loss: 0.451250] [G loss: 1.250183]\n",
            "[Epoch 5/200] [Batch 700/938] [D loss: 0.503076] [G loss: 1.032207]\n",
            "[Epoch 5/200] [Batch 800/938] [D loss: 0.523525] [G loss: 0.869898]\n",
            "[Epoch 5/200] [Batch 900/938] [D loss: 0.551076] [G loss: 1.295705]\n",
            "[Epoch 6/200] [Batch 0/938] [D loss: 0.541183] [G loss: 1.438491]\n",
            "[Epoch 6/200] [Batch 100/938] [D loss: 0.603139] [G loss: 0.670371]\n",
            "[Epoch 6/200] [Batch 200/938] [D loss: 0.867464] [G loss: 0.304547]\n",
            "[Epoch 6/200] [Batch 300/938] [D loss: 0.525255] [G loss: 1.251259]\n",
            "[Epoch 6/200] [Batch 400/938] [D loss: 0.520555] [G loss: 0.895614]\n",
            "[Epoch 6/200] [Batch 500/938] [D loss: 0.569494] [G loss: 0.866852]\n",
            "[Epoch 6/200] [Batch 600/938] [D loss: 0.602721] [G loss: 1.492038]\n",
            "[Epoch 6/200] [Batch 700/938] [D loss: 0.648260] [G loss: 1.820693]\n",
            "[Epoch 6/200] [Batch 800/938] [D loss: 0.731388] [G loss: 1.615563]\n",
            "[Epoch 6/200] [Batch 900/938] [D loss: 0.582134] [G loss: 0.844006]\n",
            "[Epoch 7/200] [Batch 0/938] [D loss: 0.624257] [G loss: 1.663590]\n",
            "[Epoch 7/200] [Batch 100/938] [D loss: 0.559588] [G loss: 1.050331]\n",
            "[Epoch 7/200] [Batch 200/938] [D loss: 0.574408] [G loss: 1.035813]\n",
            "[Epoch 7/200] [Batch 300/938] [D loss: 0.539916] [G loss: 1.184082]\n",
            "[Epoch 7/200] [Batch 400/938] [D loss: 0.530202] [G loss: 1.427140]\n",
            "[Epoch 7/200] [Batch 500/938] [D loss: 0.579390] [G loss: 0.789368]\n",
            "[Epoch 7/200] [Batch 600/938] [D loss: 0.667274] [G loss: 1.871723]\n",
            "[Epoch 7/200] [Batch 700/938] [D loss: 0.711965] [G loss: 2.066459]\n",
            "[Epoch 7/200] [Batch 800/938] [D loss: 0.525751] [G loss: 1.100819]\n",
            "[Epoch 7/200] [Batch 900/938] [D loss: 0.586297] [G loss: 0.826647]\n",
            "[Epoch 8/200] [Batch 0/938] [D loss: 0.610435] [G loss: 0.916548]\n",
            "[Epoch 8/200] [Batch 100/938] [D loss: 0.563164] [G loss: 0.815108]\n",
            "[Epoch 8/200] [Batch 200/938] [D loss: 0.589982] [G loss: 1.269554]\n",
            "[Epoch 8/200] [Batch 300/938] [D loss: 0.663505] [G loss: 1.736318]\n",
            "[Epoch 8/200] [Batch 400/938] [D loss: 0.610294] [G loss: 1.321997]\n",
            "[Epoch 8/200] [Batch 500/938] [D loss: 0.632921] [G loss: 0.755391]\n",
            "[Epoch 8/200] [Batch 600/938] [D loss: 0.587447] [G loss: 0.669114]\n",
            "[Epoch 8/200] [Batch 700/938] [D loss: 0.539775] [G loss: 1.006399]\n",
            "[Epoch 8/200] [Batch 800/938] [D loss: 0.589587] [G loss: 0.838784]\n",
            "[Epoch 8/200] [Batch 900/938] [D loss: 0.598175] [G loss: 1.295642]\n",
            "[Epoch 9/200] [Batch 0/938] [D loss: 0.685274] [G loss: 1.541570]\n",
            "[Epoch 9/200] [Batch 100/938] [D loss: 0.548474] [G loss: 1.095452]\n",
            "[Epoch 9/200] [Batch 200/938] [D loss: 0.588113] [G loss: 1.508862]\n",
            "[Epoch 9/200] [Batch 300/938] [D loss: 0.566315] [G loss: 0.761628]\n",
            "[Epoch 9/200] [Batch 400/938] [D loss: 0.596446] [G loss: 1.112239]\n",
            "[Epoch 9/200] [Batch 500/938] [D loss: 0.683690] [G loss: 1.668237]\n",
            "[Epoch 9/200] [Batch 600/938] [D loss: 0.615205] [G loss: 1.866218]\n",
            "[Epoch 9/200] [Batch 700/938] [D loss: 0.542900] [G loss: 1.204561]\n",
            "[Epoch 9/200] [Batch 800/938] [D loss: 0.655525] [G loss: 0.627042]\n",
            "[Epoch 9/200] [Batch 900/938] [D loss: 0.530735] [G loss: 1.133041]\n",
            "[Epoch 10/200] [Batch 0/938] [D loss: 0.617285] [G loss: 1.149706]\n",
            "[Epoch 10/200] [Batch 100/938] [D loss: 0.692102] [G loss: 1.627279]\n",
            "[Epoch 10/200] [Batch 200/938] [D loss: 0.661360] [G loss: 1.173295]\n",
            "[Epoch 10/200] [Batch 300/938] [D loss: 0.640452] [G loss: 1.168589]\n",
            "[Epoch 10/200] [Batch 400/938] [D loss: 0.623853] [G loss: 1.754858]\n",
            "[Epoch 10/200] [Batch 500/938] [D loss: 0.610053] [G loss: 1.097267]\n",
            "[Epoch 10/200] [Batch 600/938] [D loss: 0.682151] [G loss: 1.396810]\n",
            "[Epoch 10/200] [Batch 700/938] [D loss: 0.633193] [G loss: 1.005395]\n",
            "[Epoch 10/200] [Batch 800/938] [D loss: 0.600854] [G loss: 1.209779]\n",
            "[Epoch 10/200] [Batch 900/938] [D loss: 0.587211] [G loss: 1.165887]\n",
            "[Epoch 11/200] [Batch 0/938] [D loss: 0.514267] [G loss: 1.038395]\n",
            "[Epoch 11/200] [Batch 100/938] [D loss: 0.600845] [G loss: 0.814708]\n",
            "[Epoch 11/200] [Batch 200/938] [D loss: 0.597461] [G loss: 0.622627]\n",
            "[Epoch 11/200] [Batch 300/938] [D loss: 0.561548] [G loss: 0.940248]\n",
            "[Epoch 11/200] [Batch 400/938] [D loss: 0.586559] [G loss: 1.079837]\n",
            "[Epoch 11/200] [Batch 500/938] [D loss: 0.539348] [G loss: 1.344307]\n",
            "[Epoch 11/200] [Batch 600/938] [D loss: 0.666569] [G loss: 0.607197]\n",
            "[Epoch 11/200] [Batch 700/938] [D loss: 0.610505] [G loss: 1.256650]\n",
            "[Epoch 11/200] [Batch 800/938] [D loss: 0.677721] [G loss: 0.471744]\n",
            "[Epoch 11/200] [Batch 900/938] [D loss: 0.613935] [G loss: 1.081711]\n",
            "[Epoch 12/200] [Batch 0/938] [D loss: 0.593528] [G loss: 0.843376]\n",
            "[Epoch 12/200] [Batch 100/938] [D loss: 0.648078] [G loss: 1.288877]\n",
            "[Epoch 12/200] [Batch 200/938] [D loss: 0.572637] [G loss: 1.003013]\n",
            "[Epoch 12/200] [Batch 300/938] [D loss: 0.628963] [G loss: 1.267653]\n",
            "[Epoch 12/200] [Batch 400/938] [D loss: 0.534462] [G loss: 1.129205]\n",
            "[Epoch 12/200] [Batch 500/938] [D loss: 0.565845] [G loss: 0.768488]\n",
            "[Epoch 12/200] [Batch 600/938] [D loss: 0.563036] [G loss: 0.902515]\n",
            "[Epoch 12/200] [Batch 700/938] [D loss: 0.560142] [G loss: 0.892555]\n",
            "[Epoch 12/200] [Batch 800/938] [D loss: 0.596184] [G loss: 1.091701]\n",
            "[Epoch 12/200] [Batch 900/938] [D loss: 0.653507] [G loss: 0.958325]\n",
            "[Epoch 13/200] [Batch 0/938] [D loss: 0.636832] [G loss: 0.945140]\n",
            "[Epoch 13/200] [Batch 100/938] [D loss: 0.720051] [G loss: 1.971706]\n",
            "[Epoch 13/200] [Batch 200/938] [D loss: 0.634904] [G loss: 1.076905]\n",
            "[Epoch 13/200] [Batch 300/938] [D loss: 0.558161] [G loss: 1.347544]\n",
            "[Epoch 13/200] [Batch 400/938] [D loss: 0.631739] [G loss: 1.262981]\n",
            "[Epoch 13/200] [Batch 500/938] [D loss: 0.617836] [G loss: 1.013938]\n",
            "[Epoch 13/200] [Batch 600/938] [D loss: 0.636265] [G loss: 0.669408]\n",
            "[Epoch 13/200] [Batch 700/938] [D loss: 0.631251] [G loss: 1.366880]\n",
            "[Epoch 13/200] [Batch 800/938] [D loss: 0.573845] [G loss: 0.861024]\n",
            "[Epoch 13/200] [Batch 900/938] [D loss: 0.653593] [G loss: 1.209819]\n",
            "[Epoch 14/200] [Batch 0/938] [D loss: 0.584551] [G loss: 1.181230]\n",
            "[Epoch 14/200] [Batch 100/938] [D loss: 0.621773] [G loss: 0.779998]\n",
            "[Epoch 14/200] [Batch 200/938] [D loss: 0.547432] [G loss: 1.156047]\n",
            "[Epoch 14/200] [Batch 300/938] [D loss: 0.559437] [G loss: 0.828640]\n",
            "[Epoch 14/200] [Batch 400/938] [D loss: 0.612803] [G loss: 0.689272]\n",
            "[Epoch 14/200] [Batch 500/938] [D loss: 0.552457] [G loss: 0.886612]\n",
            "[Epoch 14/200] [Batch 600/938] [D loss: 0.659675] [G loss: 0.940758]\n",
            "[Epoch 14/200] [Batch 700/938] [D loss: 0.589045] [G loss: 0.826588]\n",
            "[Epoch 14/200] [Batch 800/938] [D loss: 0.607497] [G loss: 1.388029]\n",
            "[Epoch 14/200] [Batch 900/938] [D loss: 0.604145] [G loss: 1.071749]\n",
            "[Epoch 15/200] [Batch 0/938] [D loss: 0.587679] [G loss: 1.252915]\n",
            "[Epoch 15/200] [Batch 100/938] [D loss: 0.547753] [G loss: 0.946091]\n",
            "[Epoch 15/200] [Batch 200/938] [D loss: 0.623818] [G loss: 0.735459]\n",
            "[Epoch 15/200] [Batch 300/938] [D loss: 0.627166] [G loss: 0.911811]\n",
            "[Epoch 15/200] [Batch 400/938] [D loss: 0.635215] [G loss: 0.778922]\n",
            "[Epoch 15/200] [Batch 500/938] [D loss: 0.602110] [G loss: 1.183122]\n",
            "[Epoch 15/200] [Batch 600/938] [D loss: 0.630692] [G loss: 1.121381]\n",
            "[Epoch 15/200] [Batch 700/938] [D loss: 0.593774] [G loss: 0.975630]\n",
            "[Epoch 15/200] [Batch 800/938] [D loss: 0.593337] [G loss: 1.198686]\n",
            "[Epoch 15/200] [Batch 900/938] [D loss: 0.639343] [G loss: 1.090425]\n",
            "[Epoch 16/200] [Batch 0/938] [D loss: 0.629953] [G loss: 0.916248]\n",
            "[Epoch 16/200] [Batch 100/938] [D loss: 0.557482] [G loss: 0.894858]\n",
            "[Epoch 16/200] [Batch 200/938] [D loss: 0.536383] [G loss: 0.927484]\n",
            "[Epoch 16/200] [Batch 300/938] [D loss: 0.600989] [G loss: 0.773161]\n",
            "[Epoch 16/200] [Batch 400/938] [D loss: 0.659425] [G loss: 1.004075]\n",
            "[Epoch 16/200] [Batch 500/938] [D loss: 0.649729] [G loss: 1.036665]\n",
            "[Epoch 16/200] [Batch 600/938] [D loss: 0.583914] [G loss: 1.183225]\n",
            "[Epoch 16/200] [Batch 700/938] [D loss: 0.655976] [G loss: 0.650923]\n",
            "[Epoch 16/200] [Batch 800/938] [D loss: 0.594329] [G loss: 0.670508]\n",
            "[Epoch 16/200] [Batch 900/938] [D loss: 0.628127] [G loss: 1.134182]\n",
            "[Epoch 17/200] [Batch 0/938] [D loss: 0.581112] [G loss: 0.826408]\n",
            "[Epoch 17/200] [Batch 100/938] [D loss: 0.587174] [G loss: 1.240040]\n",
            "[Epoch 17/200] [Batch 200/938] [D loss: 0.628626] [G loss: 1.036342]\n",
            "[Epoch 17/200] [Batch 300/938] [D loss: 0.600740] [G loss: 1.304438]\n",
            "[Epoch 17/200] [Batch 400/938] [D loss: 0.715657] [G loss: 1.593416]\n",
            "[Epoch 17/200] [Batch 500/938] [D loss: 0.614507] [G loss: 1.053857]\n",
            "[Epoch 17/200] [Batch 600/938] [D loss: 0.619905] [G loss: 1.070210]\n",
            "[Epoch 17/200] [Batch 700/938] [D loss: 0.634395] [G loss: 1.030386]\n",
            "[Epoch 17/200] [Batch 800/938] [D loss: 0.611774] [G loss: 0.781089]\n",
            "[Epoch 17/200] [Batch 900/938] [D loss: 0.723068] [G loss: 0.508705]\n",
            "[Epoch 18/200] [Batch 0/938] [D loss: 0.538653] [G loss: 1.049718]\n",
            "[Epoch 18/200] [Batch 100/938] [D loss: 0.661224] [G loss: 1.375581]\n",
            "[Epoch 18/200] [Batch 200/938] [D loss: 0.583611] [G loss: 0.814617]\n",
            "[Epoch 18/200] [Batch 300/938] [D loss: 0.654232] [G loss: 1.014881]\n",
            "[Epoch 18/200] [Batch 400/938] [D loss: 0.561104] [G loss: 1.040589]\n",
            "[Epoch 18/200] [Batch 500/938] [D loss: 0.582121] [G loss: 0.827405]\n",
            "[Epoch 18/200] [Batch 600/938] [D loss: 0.654596] [G loss: 1.171581]\n",
            "[Epoch 18/200] [Batch 700/938] [D loss: 0.614196] [G loss: 0.834093]\n",
            "[Epoch 18/200] [Batch 800/938] [D loss: 0.624678] [G loss: 0.925179]\n",
            "[Epoch 18/200] [Batch 900/938] [D loss: 0.590788] [G loss: 0.812717]\n",
            "[Epoch 19/200] [Batch 0/938] [D loss: 0.608933] [G loss: 0.993886]\n",
            "[Epoch 19/200] [Batch 100/938] [D loss: 0.614401] [G loss: 0.769364]\n",
            "[Epoch 19/200] [Batch 200/938] [D loss: 0.634570] [G loss: 1.002825]\n",
            "[Epoch 19/200] [Batch 300/938] [D loss: 0.805218] [G loss: 0.358702]\n",
            "[Epoch 19/200] [Batch 400/938] [D loss: 0.552465] [G loss: 1.082287]\n",
            "[Epoch 19/200] [Batch 500/938] [D loss: 0.564621] [G loss: 0.937514]\n",
            "[Epoch 19/200] [Batch 600/938] [D loss: 0.639590] [G loss: 1.015286]\n",
            "[Epoch 19/200] [Batch 700/938] [D loss: 0.647309] [G loss: 1.194232]\n",
            "[Epoch 19/200] [Batch 800/938] [D loss: 0.622945] [G loss: 1.547285]\n",
            "[Epoch 19/200] [Batch 900/938] [D loss: 0.581189] [G loss: 0.875517]\n",
            "[Epoch 20/200] [Batch 0/938] [D loss: 0.565982] [G loss: 0.731103]\n",
            "[Epoch 20/200] [Batch 100/938] [D loss: 0.632723] [G loss: 0.790659]\n",
            "[Epoch 20/200] [Batch 200/938] [D loss: 0.610162] [G loss: 0.908076]\n",
            "[Epoch 20/200] [Batch 300/938] [D loss: 0.658149] [G loss: 1.530617]\n",
            "[Epoch 20/200] [Batch 400/938] [D loss: 0.583646] [G loss: 0.972977]\n",
            "[Epoch 20/200] [Batch 500/938] [D loss: 0.591451] [G loss: 1.075897]\n",
            "[Epoch 20/200] [Batch 600/938] [D loss: 0.634712] [G loss: 1.029380]\n",
            "[Epoch 20/200] [Batch 700/938] [D loss: 0.652500] [G loss: 0.914607]\n",
            "[Epoch 20/200] [Batch 800/938] [D loss: 0.706709] [G loss: 0.576808]\n",
            "[Epoch 20/200] [Batch 900/938] [D loss: 0.631651] [G loss: 0.589767]\n",
            "[Epoch 21/200] [Batch 0/938] [D loss: 0.673492] [G loss: 0.625202]\n",
            "[Epoch 21/200] [Batch 100/938] [D loss: 0.634277] [G loss: 1.005702]\n",
            "[Epoch 21/200] [Batch 200/938] [D loss: 0.592914] [G loss: 0.825643]\n",
            "[Epoch 21/200] [Batch 300/938] [D loss: 0.572890] [G loss: 1.308872]\n",
            "[Epoch 21/200] [Batch 400/938] [D loss: 0.631958] [G loss: 0.963296]\n",
            "[Epoch 21/200] [Batch 500/938] [D loss: 0.650879] [G loss: 1.095381]\n",
            "[Epoch 21/200] [Batch 600/938] [D loss: 0.591122] [G loss: 1.086150]\n",
            "[Epoch 21/200] [Batch 700/938] [D loss: 0.582269] [G loss: 0.883764]\n",
            "[Epoch 21/200] [Batch 800/938] [D loss: 0.619910] [G loss: 0.906943]\n",
            "[Epoch 21/200] [Batch 900/938] [D loss: 0.590107] [G loss: 0.932551]\n",
            "[Epoch 22/200] [Batch 0/938] [D loss: 0.671313] [G loss: 0.892624]\n",
            "[Epoch 22/200] [Batch 100/938] [D loss: 0.584933] [G loss: 0.788896]\n",
            "[Epoch 22/200] [Batch 200/938] [D loss: 0.587874] [G loss: 1.133497]\n",
            "[Epoch 22/200] [Batch 300/938] [D loss: 0.557803] [G loss: 0.985067]\n",
            "[Epoch 22/200] [Batch 400/938] [D loss: 0.632531] [G loss: 1.168652]\n",
            "[Epoch 22/200] [Batch 500/938] [D loss: 0.597631] [G loss: 0.731693]\n",
            "[Epoch 22/200] [Batch 600/938] [D loss: 0.617264] [G loss: 0.804595]\n",
            "[Epoch 22/200] [Batch 700/938] [D loss: 0.693138] [G loss: 1.131901]\n",
            "[Epoch 22/200] [Batch 800/938] [D loss: 0.617740] [G loss: 0.934864]\n",
            "[Epoch 22/200] [Batch 900/938] [D loss: 0.666809] [G loss: 0.621634]\n",
            "[Epoch 23/200] [Batch 0/938] [D loss: 0.564204] [G loss: 0.930313]\n",
            "[Epoch 23/200] [Batch 100/938] [D loss: 0.646959] [G loss: 0.873502]\n",
            "[Epoch 23/200] [Batch 200/938] [D loss: 0.689486] [G loss: 0.571737]\n",
            "[Epoch 23/200] [Batch 300/938] [D loss: 0.598578] [G loss: 0.848549]\n",
            "[Epoch 23/200] [Batch 400/938] [D loss: 0.583177] [G loss: 0.746944]\n",
            "[Epoch 23/200] [Batch 500/938] [D loss: 0.629142] [G loss: 0.978280]\n",
            "[Epoch 23/200] [Batch 600/938] [D loss: 0.662091] [G loss: 0.779238]\n",
            "[Epoch 23/200] [Batch 700/938] [D loss: 0.709088] [G loss: 0.718493]\n",
            "[Epoch 23/200] [Batch 800/938] [D loss: 0.623612] [G loss: 0.658468]\n",
            "[Epoch 23/200] [Batch 900/938] [D loss: 0.687939] [G loss: 0.878059]\n",
            "[Epoch 24/200] [Batch 0/938] [D loss: 0.640378] [G loss: 0.680324]\n",
            "[Epoch 24/200] [Batch 100/938] [D loss: 0.639551] [G loss: 1.048647]\n",
            "[Epoch 24/200] [Batch 200/938] [D loss: 0.596094] [G loss: 0.734473]\n",
            "[Epoch 24/200] [Batch 300/938] [D loss: 0.672119] [G loss: 0.504315]\n",
            "[Epoch 24/200] [Batch 400/938] [D loss: 0.640429] [G loss: 0.634955]\n",
            "[Epoch 24/200] [Batch 500/938] [D loss: 0.623935] [G loss: 0.975288]\n",
            "[Epoch 24/200] [Batch 600/938] [D loss: 0.646613] [G loss: 0.832690]\n",
            "[Epoch 24/200] [Batch 700/938] [D loss: 0.655479] [G loss: 0.628708]\n",
            "[Epoch 24/200] [Batch 800/938] [D loss: 0.663222] [G loss: 0.854171]\n",
            "[Epoch 24/200] [Batch 900/938] [D loss: 0.612135] [G loss: 0.754591]\n",
            "[Epoch 25/200] [Batch 0/938] [D loss: 0.629294] [G loss: 0.863781]\n",
            "[Epoch 25/200] [Batch 100/938] [D loss: 0.625661] [G loss: 0.918549]\n",
            "[Epoch 25/200] [Batch 200/938] [D loss: 0.599647] [G loss: 0.909457]\n",
            "[Epoch 25/200] [Batch 300/938] [D loss: 0.672079] [G loss: 0.675795]\n",
            "[Epoch 25/200] [Batch 400/938] [D loss: 0.658163] [G loss: 0.931769]\n",
            "[Epoch 25/200] [Batch 500/938] [D loss: 0.658810] [G loss: 0.739319]\n",
            "[Epoch 25/200] [Batch 600/938] [D loss: 0.638158] [G loss: 1.340057]\n",
            "[Epoch 25/200] [Batch 700/938] [D loss: 0.629816] [G loss: 1.111691]\n",
            "[Epoch 25/200] [Batch 800/938] [D loss: 0.702133] [G loss: 1.356033]\n",
            "[Epoch 25/200] [Batch 900/938] [D loss: 0.582272] [G loss: 1.168696]\n",
            "[Epoch 26/200] [Batch 0/938] [D loss: 0.611128] [G loss: 0.824809]\n",
            "[Epoch 26/200] [Batch 100/938] [D loss: 0.679478] [G loss: 1.205874]\n",
            "[Epoch 26/200] [Batch 200/938] [D loss: 0.616528] [G loss: 0.777237]\n",
            "[Epoch 26/200] [Batch 300/938] [D loss: 0.609502] [G loss: 1.052826]\n",
            "[Epoch 26/200] [Batch 400/938] [D loss: 0.606618] [G loss: 0.816283]\n",
            "[Epoch 26/200] [Batch 500/938] [D loss: 0.636279] [G loss: 0.724555]\n",
            "[Epoch 26/200] [Batch 600/938] [D loss: 0.614302] [G loss: 0.855125]\n",
            "[Epoch 26/200] [Batch 700/938] [D loss: 0.568196] [G loss: 1.303784]\n",
            "[Epoch 26/200] [Batch 800/938] [D loss: 0.610082] [G loss: 0.911700]\n",
            "[Epoch 26/200] [Batch 900/938] [D loss: 0.662672] [G loss: 0.906245]\n",
            "[Epoch 27/200] [Batch 0/938] [D loss: 0.603861] [G loss: 1.034670]\n",
            "[Epoch 27/200] [Batch 100/938] [D loss: 0.664591] [G loss: 0.600741]\n",
            "[Epoch 27/200] [Batch 200/938] [D loss: 0.621492] [G loss: 1.137450]\n",
            "[Epoch 27/200] [Batch 300/938] [D loss: 0.685096] [G loss: 0.531862]\n",
            "[Epoch 27/200] [Batch 400/938] [D loss: 0.631800] [G loss: 1.214696]\n",
            "[Epoch 27/200] [Batch 500/938] [D loss: 0.573650] [G loss: 1.242594]\n",
            "[Epoch 27/200] [Batch 600/938] [D loss: 0.641952] [G loss: 1.010461]\n",
            "[Epoch 27/200] [Batch 700/938] [D loss: 0.640800] [G loss: 1.019405]\n",
            "[Epoch 27/200] [Batch 800/938] [D loss: 0.601515] [G loss: 0.728751]\n",
            "[Epoch 27/200] [Batch 900/938] [D loss: 0.682059] [G loss: 0.533851]\n",
            "[Epoch 28/200] [Batch 0/938] [D loss: 0.596105] [G loss: 0.951344]\n",
            "[Epoch 28/200] [Batch 100/938] [D loss: 0.591897] [G loss: 0.696813]\n",
            "[Epoch 28/200] [Batch 200/938] [D loss: 0.634611] [G loss: 1.077567]\n",
            "[Epoch 28/200] [Batch 300/938] [D loss: 0.635575] [G loss: 0.908046]\n",
            "[Epoch 28/200] [Batch 400/938] [D loss: 0.656417] [G loss: 0.763884]\n",
            "[Epoch 28/200] [Batch 500/938] [D loss: 0.590058] [G loss: 0.857204]\n",
            "[Epoch 28/200] [Batch 600/938] [D loss: 0.588322] [G loss: 1.011741]\n",
            "[Epoch 28/200] [Batch 700/938] [D loss: 0.627273] [G loss: 1.125549]\n",
            "[Epoch 28/200] [Batch 800/938] [D loss: 0.614329] [G loss: 0.684917]\n",
            "[Epoch 28/200] [Batch 900/938] [D loss: 0.640605] [G loss: 0.997820]\n",
            "[Epoch 29/200] [Batch 0/938] [D loss: 0.598265] [G loss: 0.786679]\n",
            "[Epoch 29/200] [Batch 100/938] [D loss: 0.635968] [G loss: 0.844484]\n",
            "[Epoch 29/200] [Batch 200/938] [D loss: 0.658350] [G loss: 0.958457]\n",
            "[Epoch 29/200] [Batch 300/938] [D loss: 0.646768] [G loss: 0.796058]\n",
            "[Epoch 29/200] [Batch 400/938] [D loss: 0.632523] [G loss: 0.966793]\n",
            "[Epoch 29/200] [Batch 500/938] [D loss: 0.642589] [G loss: 0.988481]\n",
            "[Epoch 29/200] [Batch 600/938] [D loss: 0.626915] [G loss: 0.780120]\n",
            "[Epoch 29/200] [Batch 700/938] [D loss: 0.616522] [G loss: 0.946288]\n",
            "[Epoch 29/200] [Batch 800/938] [D loss: 0.568371] [G loss: 0.941313]\n",
            "[Epoch 29/200] [Batch 900/938] [D loss: 0.640428] [G loss: 0.587786]\n",
            "[Epoch 30/200] [Batch 0/938] [D loss: 0.684743] [G loss: 1.162939]\n",
            "[Epoch 30/200] [Batch 100/938] [D loss: 0.620427] [G loss: 0.962610]\n",
            "[Epoch 30/200] [Batch 200/938] [D loss: 0.651487] [G loss: 0.847309]\n",
            "[Epoch 30/200] [Batch 300/938] [D loss: 0.591689] [G loss: 0.958254]\n",
            "[Epoch 30/200] [Batch 400/938] [D loss: 0.607053] [G loss: 0.938453]\n",
            "[Epoch 30/200] [Batch 500/938] [D loss: 0.604556] [G loss: 1.111497]\n",
            "[Epoch 30/200] [Batch 600/938] [D loss: 0.570094] [G loss: 1.009379]\n",
            "[Epoch 30/200] [Batch 700/938] [D loss: 0.626409] [G loss: 1.117711]\n",
            "[Epoch 30/200] [Batch 800/938] [D loss: 0.592811] [G loss: 0.960160]\n",
            "[Epoch 30/200] [Batch 900/938] [D loss: 0.623756] [G loss: 0.870531]\n",
            "[Epoch 31/200] [Batch 0/938] [D loss: 0.608714] [G loss: 0.797790]\n",
            "[Epoch 31/200] [Batch 100/938] [D loss: 0.587805] [G loss: 1.398399]\n",
            "[Epoch 31/200] [Batch 200/938] [D loss: 0.594905] [G loss: 0.935274]\n",
            "[Epoch 31/200] [Batch 300/938] [D loss: 0.545128] [G loss: 0.863740]\n",
            "[Epoch 31/200] [Batch 400/938] [D loss: 0.638426] [G loss: 1.004055]\n",
            "[Epoch 31/200] [Batch 500/938] [D loss: 0.613747] [G loss: 1.092499]\n",
            "[Epoch 31/200] [Batch 600/938] [D loss: 0.635640] [G loss: 0.940986]\n",
            "[Epoch 31/200] [Batch 700/938] [D loss: 0.647190] [G loss: 0.949171]\n",
            "[Epoch 31/200] [Batch 800/938] [D loss: 0.618384] [G loss: 1.102692]\n",
            "[Epoch 31/200] [Batch 900/938] [D loss: 0.634304] [G loss: 0.845096]\n",
            "[Epoch 32/200] [Batch 0/938] [D loss: 0.630042] [G loss: 1.399479]\n",
            "[Epoch 32/200] [Batch 100/938] [D loss: 0.649186] [G loss: 0.646711]\n",
            "[Epoch 32/200] [Batch 200/938] [D loss: 0.683736] [G loss: 0.711206]\n",
            "[Epoch 32/200] [Batch 300/938] [D loss: 0.641047] [G loss: 1.283589]\n",
            "[Epoch 32/200] [Batch 400/938] [D loss: 0.607355] [G loss: 1.126228]\n",
            "[Epoch 32/200] [Batch 500/938] [D loss: 0.614807] [G loss: 1.004128]\n",
            "[Epoch 32/200] [Batch 600/938] [D loss: 0.615608] [G loss: 0.746000]\n",
            "[Epoch 32/200] [Batch 700/938] [D loss: 0.591367] [G loss: 0.844853]\n",
            "[Epoch 32/200] [Batch 800/938] [D loss: 0.608608] [G loss: 1.075554]\n",
            "[Epoch 32/200] [Batch 900/938] [D loss: 0.670076] [G loss: 0.816747]\n",
            "[Epoch 33/200] [Batch 0/938] [D loss: 0.655970] [G loss: 0.821284]\n",
            "[Epoch 33/200] [Batch 100/938] [D loss: 0.692730] [G loss: 1.120354]\n",
            "[Epoch 33/200] [Batch 200/938] [D loss: 0.585706] [G loss: 0.975570]\n",
            "[Epoch 33/200] [Batch 300/938] [D loss: 0.644162] [G loss: 0.728876]\n",
            "[Epoch 33/200] [Batch 400/938] [D loss: 0.561003] [G loss: 0.976075]\n",
            "[Epoch 33/200] [Batch 500/938] [D loss: 0.632261] [G loss: 0.792937]\n",
            "[Epoch 33/200] [Batch 600/938] [D loss: 0.629492] [G loss: 1.143302]\n",
            "[Epoch 33/200] [Batch 700/938] [D loss: 0.607092] [G loss: 0.831084]\n",
            "[Epoch 33/200] [Batch 800/938] [D loss: 0.618356] [G loss: 0.800998]\n",
            "[Epoch 33/200] [Batch 900/938] [D loss: 0.709572] [G loss: 0.505263]\n",
            "[Epoch 34/200] [Batch 0/938] [D loss: 0.598476] [G loss: 0.980836]\n",
            "[Epoch 34/200] [Batch 100/938] [D loss: 0.625272] [G loss: 0.911068]\n",
            "[Epoch 34/200] [Batch 200/938] [D loss: 0.669291] [G loss: 1.070506]\n",
            "[Epoch 34/200] [Batch 300/938] [D loss: 0.625543] [G loss: 1.123695]\n",
            "[Epoch 34/200] [Batch 400/938] [D loss: 0.635904] [G loss: 1.394258]\n",
            "[Epoch 34/200] [Batch 500/938] [D loss: 0.616002] [G loss: 0.834586]\n",
            "[Epoch 34/200] [Batch 600/938] [D loss: 0.575545] [G loss: 1.104496]\n",
            "[Epoch 34/200] [Batch 700/938] [D loss: 0.642366] [G loss: 1.167851]\n",
            "[Epoch 34/200] [Batch 800/938] [D loss: 0.678402] [G loss: 0.691118]\n",
            "[Epoch 34/200] [Batch 900/938] [D loss: 0.631197] [G loss: 0.893425]\n",
            "[Epoch 35/200] [Batch 0/938] [D loss: 0.579343] [G loss: 0.880885]\n",
            "[Epoch 35/200] [Batch 100/938] [D loss: 0.601418] [G loss: 1.076414]\n",
            "[Epoch 35/200] [Batch 200/938] [D loss: 0.550866] [G loss: 0.920987]\n",
            "[Epoch 35/200] [Batch 300/938] [D loss: 0.602726] [G loss: 0.997654]\n",
            "[Epoch 35/200] [Batch 400/938] [D loss: 0.646929] [G loss: 0.711109]\n",
            "[Epoch 35/200] [Batch 500/938] [D loss: 0.590439] [G loss: 0.942782]\n",
            "[Epoch 35/200] [Batch 600/938] [D loss: 0.595273] [G loss: 0.885800]\n",
            "[Epoch 35/200] [Batch 700/938] [D loss: 0.634741] [G loss: 1.217369]\n",
            "[Epoch 35/200] [Batch 800/938] [D loss: 0.670207] [G loss: 1.311576]\n",
            "[Epoch 35/200] [Batch 900/938] [D loss: 0.593255] [G loss: 1.157101]\n",
            "[Epoch 36/200] [Batch 0/938] [D loss: 0.664328] [G loss: 1.225246]\n",
            "[Epoch 36/200] [Batch 100/938] [D loss: 0.627006] [G loss: 1.096845]\n",
            "[Epoch 36/200] [Batch 200/938] [D loss: 0.600602] [G loss: 1.114793]\n",
            "[Epoch 36/200] [Batch 300/938] [D loss: 0.567397] [G loss: 1.162387]\n",
            "[Epoch 36/200] [Batch 400/938] [D loss: 0.625848] [G loss: 1.139260]\n",
            "[Epoch 36/200] [Batch 500/938] [D loss: 0.661597] [G loss: 1.301193]\n",
            "[Epoch 36/200] [Batch 600/938] [D loss: 0.612153] [G loss: 1.015000]\n",
            "[Epoch 36/200] [Batch 700/938] [D loss: 0.611709] [G loss: 1.050937]\n",
            "[Epoch 36/200] [Batch 800/938] [D loss: 0.624007] [G loss: 0.995435]\n",
            "[Epoch 36/200] [Batch 900/938] [D loss: 0.613308] [G loss: 1.338313]\n",
            "[Epoch 37/200] [Batch 0/938] [D loss: 0.592733] [G loss: 0.728225]\n",
            "[Epoch 37/200] [Batch 100/938] [D loss: 0.609001] [G loss: 0.953276]\n",
            "[Epoch 37/200] [Batch 200/938] [D loss: 0.617653] [G loss: 0.886350]\n",
            "[Epoch 37/200] [Batch 300/938] [D loss: 0.613259] [G loss: 0.811486]\n",
            "[Epoch 37/200] [Batch 400/938] [D loss: 0.658667] [G loss: 1.055536]\n",
            "[Epoch 37/200] [Batch 500/938] [D loss: 0.591933] [G loss: 0.939020]\n",
            "[Epoch 37/200] [Batch 600/938] [D loss: 0.590274] [G loss: 0.887008]\n",
            "[Epoch 37/200] [Batch 700/938] [D loss: 0.642035] [G loss: 0.872060]\n",
            "[Epoch 37/200] [Batch 800/938] [D loss: 0.584528] [G loss: 1.134120]\n",
            "[Epoch 37/200] [Batch 900/938] [D loss: 0.594498] [G loss: 0.906761]\n",
            "[Epoch 38/200] [Batch 0/938] [D loss: 0.630934] [G loss: 1.237416]\n",
            "[Epoch 38/200] [Batch 100/938] [D loss: 0.680084] [G loss: 0.763821]\n",
            "[Epoch 38/200] [Batch 200/938] [D loss: 0.903824] [G loss: 0.288873]\n",
            "[Epoch 38/200] [Batch 300/938] [D loss: 0.621115] [G loss: 0.716311]\n",
            "[Epoch 38/200] [Batch 400/938] [D loss: 0.612738] [G loss: 0.964410]\n",
            "[Epoch 38/200] [Batch 500/938] [D loss: 0.600880] [G loss: 0.838780]\n",
            "[Epoch 38/200] [Batch 600/938] [D loss: 0.626861] [G loss: 0.743926]\n",
            "[Epoch 38/200] [Batch 700/938] [D loss: 0.622480] [G loss: 0.841588]\n",
            "[Epoch 38/200] [Batch 800/938] [D loss: 0.618971] [G loss: 1.088397]\n",
            "[Epoch 38/200] [Batch 900/938] [D loss: 0.666561] [G loss: 0.790576]\n",
            "[Epoch 39/200] [Batch 0/938] [D loss: 0.593929] [G loss: 0.778107]\n",
            "[Epoch 39/200] [Batch 100/938] [D loss: 0.623659] [G loss: 0.779076]\n",
            "[Epoch 39/200] [Batch 200/938] [D loss: 0.592591] [G loss: 1.120965]\n",
            "[Epoch 39/200] [Batch 300/938] [D loss: 0.689976] [G loss: 0.648153]\n",
            "[Epoch 39/200] [Batch 400/938] [D loss: 0.601766] [G loss: 0.867392]\n",
            "[Epoch 39/200] [Batch 500/938] [D loss: 0.603737] [G loss: 0.906368]\n",
            "[Epoch 39/200] [Batch 600/938] [D loss: 0.620029] [G loss: 0.742572]\n",
            "[Epoch 39/200] [Batch 700/938] [D loss: 0.597321] [G loss: 0.968813]\n",
            "[Epoch 39/200] [Batch 800/938] [D loss: 0.597446] [G loss: 1.010689]\n",
            "[Epoch 39/200] [Batch 900/938] [D loss: 0.596960] [G loss: 0.946435]\n",
            "[Epoch 40/200] [Batch 0/938] [D loss: 0.752809] [G loss: 0.493319]\n",
            "[Epoch 40/200] [Batch 100/938] [D loss: 0.590641] [G loss: 1.202888]\n",
            "[Epoch 40/200] [Batch 200/938] [D loss: 0.672350] [G loss: 0.743943]\n",
            "[Epoch 40/200] [Batch 300/938] [D loss: 0.580663] [G loss: 1.051970]\n",
            "[Epoch 40/200] [Batch 400/938] [D loss: 0.612763] [G loss: 0.794954]\n",
            "[Epoch 40/200] [Batch 500/938] [D loss: 0.602414] [G loss: 1.007435]\n",
            "[Epoch 40/200] [Batch 600/938] [D loss: 0.647705] [G loss: 0.681734]\n",
            "[Epoch 40/200] [Batch 700/938] [D loss: 0.681923] [G loss: 0.567101]\n",
            "[Epoch 40/200] [Batch 800/938] [D loss: 0.622620] [G loss: 0.871460]\n",
            "[Epoch 40/200] [Batch 900/938] [D loss: 0.627052] [G loss: 0.962940]\n",
            "[Epoch 41/200] [Batch 0/938] [D loss: 0.691651] [G loss: 0.997556]\n",
            "[Epoch 41/200] [Batch 100/938] [D loss: 0.691312] [G loss: 1.540102]\n",
            "[Epoch 41/200] [Batch 200/938] [D loss: 0.605881] [G loss: 1.173946]\n",
            "[Epoch 41/200] [Batch 300/938] [D loss: 0.632546] [G loss: 1.062100]\n",
            "[Epoch 41/200] [Batch 400/938] [D loss: 0.650978] [G loss: 0.995296]\n",
            "[Epoch 41/200] [Batch 500/938] [D loss: 0.624239] [G loss: 0.839585]\n",
            "[Epoch 41/200] [Batch 600/938] [D loss: 0.616367] [G loss: 0.776695]\n",
            "[Epoch 41/200] [Batch 700/938] [D loss: 0.602051] [G loss: 0.755941]\n",
            "[Epoch 41/200] [Batch 800/938] [D loss: 0.682004] [G loss: 0.520627]\n",
            "[Epoch 41/200] [Batch 900/938] [D loss: 0.661903] [G loss: 1.329618]\n",
            "[Epoch 42/200] [Batch 0/938] [D loss: 0.626279] [G loss: 0.702744]\n",
            "[Epoch 42/200] [Batch 100/938] [D loss: 0.587384] [G loss: 1.005807]\n",
            "[Epoch 42/200] [Batch 200/938] [D loss: 0.635797] [G loss: 0.716523]\n",
            "[Epoch 42/200] [Batch 300/938] [D loss: 0.607323] [G loss: 0.913337]\n",
            "[Epoch 42/200] [Batch 400/938] [D loss: 0.679354] [G loss: 0.585833]\n",
            "[Epoch 42/200] [Batch 500/938] [D loss: 0.591989] [G loss: 1.118924]\n",
            "[Epoch 42/200] [Batch 600/938] [D loss: 0.611428] [G loss: 0.634362]\n",
            "[Epoch 42/200] [Batch 700/938] [D loss: 0.560725] [G loss: 1.245418]\n",
            "[Epoch 42/200] [Batch 800/938] [D loss: 0.694977] [G loss: 0.685264]\n",
            "[Epoch 42/200] [Batch 900/938] [D loss: 0.617654] [G loss: 0.871349]\n",
            "[Epoch 43/200] [Batch 0/938] [D loss: 0.577105] [G loss: 1.064109]\n",
            "[Epoch 43/200] [Batch 100/938] [D loss: 0.659307] [G loss: 0.771498]\n",
            "[Epoch 43/200] [Batch 200/938] [D loss: 0.579209] [G loss: 0.961661]\n",
            "[Epoch 43/200] [Batch 300/938] [D loss: 0.694504] [G loss: 0.633324]\n",
            "[Epoch 43/200] [Batch 400/938] [D loss: 0.658235] [G loss: 1.250957]\n",
            "[Epoch 43/200] [Batch 500/938] [D loss: 0.591547] [G loss: 1.034760]\n",
            "[Epoch 43/200] [Batch 600/938] [D loss: 0.666197] [G loss: 1.369603]\n",
            "[Epoch 43/200] [Batch 700/938] [D loss: 0.692982] [G loss: 1.130107]\n",
            "[Epoch 43/200] [Batch 800/938] [D loss: 0.637142] [G loss: 1.054294]\n",
            "[Epoch 43/200] [Batch 900/938] [D loss: 0.606091] [G loss: 1.160616]\n",
            "[Epoch 44/200] [Batch 0/938] [D loss: 0.646096] [G loss: 0.920067]\n",
            "[Epoch 44/200] [Batch 100/938] [D loss: 0.607231] [G loss: 1.097987]\n",
            "[Epoch 44/200] [Batch 200/938] [D loss: 0.617544] [G loss: 1.118595]\n",
            "[Epoch 44/200] [Batch 300/938] [D loss: 0.645082] [G loss: 0.880889]\n",
            "[Epoch 44/200] [Batch 400/938] [D loss: 0.611974] [G loss: 1.553511]\n",
            "[Epoch 44/200] [Batch 500/938] [D loss: 0.601026] [G loss: 1.018240]\n",
            "[Epoch 44/200] [Batch 600/938] [D loss: 0.597018] [G loss: 1.212478]\n",
            "[Epoch 44/200] [Batch 700/938] [D loss: 0.601548] [G loss: 1.003156]\n",
            "[Epoch 44/200] [Batch 800/938] [D loss: 0.617486] [G loss: 0.944127]\n",
            "[Epoch 44/200] [Batch 900/938] [D loss: 0.561463] [G loss: 0.973954]\n",
            "[Epoch 45/200] [Batch 0/938] [D loss: 0.647111] [G loss: 0.998612]\n",
            "[Epoch 45/200] [Batch 100/938] [D loss: 0.652640] [G loss: 1.115293]\n",
            "[Epoch 45/200] [Batch 200/938] [D loss: 0.575720] [G loss: 0.988387]\n",
            "[Epoch 45/200] [Batch 300/938] [D loss: 0.644916] [G loss: 1.328024]\n",
            "[Epoch 45/200] [Batch 400/938] [D loss: 0.616082] [G loss: 0.937023]\n",
            "[Epoch 45/200] [Batch 500/938] [D loss: 0.597462] [G loss: 1.266051]\n",
            "[Epoch 45/200] [Batch 600/938] [D loss: 0.599450] [G loss: 0.890741]\n",
            "[Epoch 45/200] [Batch 700/938] [D loss: 0.559269] [G loss: 1.092971]\n",
            "[Epoch 45/200] [Batch 800/938] [D loss: 0.573309] [G loss: 0.941236]\n",
            "[Epoch 45/200] [Batch 900/938] [D loss: 0.621916] [G loss: 1.394981]\n",
            "[Epoch 46/200] [Batch 0/938] [D loss: 0.661277] [G loss: 1.011520]\n",
            "[Epoch 46/200] [Batch 100/938] [D loss: 0.615568] [G loss: 0.941577]\n",
            "[Epoch 46/200] [Batch 200/938] [D loss: 0.621953] [G loss: 0.977728]\n",
            "[Epoch 46/200] [Batch 300/938] [D loss: 0.637776] [G loss: 0.947731]\n",
            "[Epoch 46/200] [Batch 400/938] [D loss: 0.694987] [G loss: 1.384208]\n",
            "[Epoch 46/200] [Batch 500/938] [D loss: 0.572693] [G loss: 1.150095]\n",
            "[Epoch 46/200] [Batch 600/938] [D loss: 0.674211] [G loss: 0.670118]\n",
            "[Epoch 46/200] [Batch 700/938] [D loss: 0.731684] [G loss: 1.088530]\n",
            "[Epoch 46/200] [Batch 800/938] [D loss: 0.650731] [G loss: 0.724989]\n",
            "[Epoch 46/200] [Batch 900/938] [D loss: 0.650533] [G loss: 0.653944]\n",
            "[Epoch 47/200] [Batch 0/938] [D loss: 0.573953] [G loss: 1.093256]\n",
            "[Epoch 47/200] [Batch 100/938] [D loss: 0.559688] [G loss: 0.992072]\n",
            "[Epoch 47/200] [Batch 200/938] [D loss: 0.621751] [G loss: 0.651219]\n",
            "[Epoch 47/200] [Batch 300/938] [D loss: 0.665350] [G loss: 1.018425]\n",
            "[Epoch 47/200] [Batch 400/938] [D loss: 0.682767] [G loss: 0.791644]\n",
            "[Epoch 47/200] [Batch 500/938] [D loss: 0.662901] [G loss: 1.033389]\n",
            "[Epoch 47/200] [Batch 600/938] [D loss: 0.624035] [G loss: 0.862002]\n",
            "[Epoch 47/200] [Batch 700/938] [D loss: 0.553372] [G loss: 1.085860]\n",
            "[Epoch 47/200] [Batch 800/938] [D loss: 0.630496] [G loss: 0.685395]\n",
            "[Epoch 47/200] [Batch 900/938] [D loss: 0.580570] [G loss: 1.050342]\n",
            "[Epoch 48/200] [Batch 0/938] [D loss: 0.686616] [G loss: 0.594121]\n",
            "[Epoch 48/200] [Batch 100/938] [D loss: 0.629398] [G loss: 0.579572]\n",
            "[Epoch 48/200] [Batch 200/938] [D loss: 0.635933] [G loss: 0.952888]\n",
            "[Epoch 48/200] [Batch 300/938] [D loss: 0.646447] [G loss: 0.923979]\n",
            "[Epoch 48/200] [Batch 400/938] [D loss: 0.587133] [G loss: 0.928002]\n",
            "[Epoch 48/200] [Batch 500/938] [D loss: 0.583700] [G loss: 1.138530]\n",
            "[Epoch 48/200] [Batch 600/938] [D loss: 0.626463] [G loss: 0.827391]\n",
            "[Epoch 48/200] [Batch 700/938] [D loss: 0.583293] [G loss: 0.872030]\n",
            "[Epoch 48/200] [Batch 800/938] [D loss: 0.581486] [G loss: 1.037290]\n",
            "[Epoch 48/200] [Batch 900/938] [D loss: 0.665629] [G loss: 0.749875]\n",
            "[Epoch 49/200] [Batch 0/938] [D loss: 0.603334] [G loss: 1.072606]\n",
            "[Epoch 49/200] [Batch 100/938] [D loss: 0.598960] [G loss: 1.357369]\n",
            "[Epoch 49/200] [Batch 200/938] [D loss: 0.529269] [G loss: 1.062372]\n",
            "[Epoch 49/200] [Batch 300/938] [D loss: 0.564924] [G loss: 1.235144]\n",
            "[Epoch 49/200] [Batch 400/938] [D loss: 0.611380] [G loss: 0.847731]\n",
            "[Epoch 49/200] [Batch 500/938] [D loss: 0.608556] [G loss: 1.272750]\n",
            "[Epoch 49/200] [Batch 600/938] [D loss: 0.586688] [G loss: 0.907530]\n",
            "[Epoch 49/200] [Batch 700/938] [D loss: 0.591667] [G loss: 1.335837]\n",
            "[Epoch 49/200] [Batch 800/938] [D loss: 0.613198] [G loss: 1.312346]\n",
            "[Epoch 49/200] [Batch 900/938] [D loss: 0.641176] [G loss: 0.787888]\n",
            "[Epoch 50/200] [Batch 0/938] [D loss: 0.609289] [G loss: 0.672635]\n",
            "[Epoch 50/200] [Batch 100/938] [D loss: 0.555342] [G loss: 0.884585]\n",
            "[Epoch 50/200] [Batch 200/938] [D loss: 0.625754] [G loss: 1.274774]\n",
            "[Epoch 50/200] [Batch 300/938] [D loss: 0.683737] [G loss: 0.577407]\n",
            "[Epoch 50/200] [Batch 400/938] [D loss: 0.580934] [G loss: 1.218824]\n",
            "[Epoch 50/200] [Batch 500/938] [D loss: 0.584756] [G loss: 1.244117]\n",
            "[Epoch 50/200] [Batch 600/938] [D loss: 0.640025] [G loss: 0.693399]\n",
            "[Epoch 50/200] [Batch 700/938] [D loss: 0.656839] [G loss: 1.288336]\n",
            "[Epoch 50/200] [Batch 800/938] [D loss: 0.781399] [G loss: 0.416749]\n",
            "[Epoch 50/200] [Batch 900/938] [D loss: 0.549415] [G loss: 0.980987]\n",
            "[Epoch 51/200] [Batch 0/938] [D loss: 0.596495] [G loss: 0.909196]\n",
            "[Epoch 51/200] [Batch 100/938] [D loss: 0.641767] [G loss: 1.404289]\n",
            "[Epoch 51/200] [Batch 200/938] [D loss: 0.611116] [G loss: 0.956062]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-94694b58c7e0>\u001b[0m in \u001b[0;36m<cell line: 117>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Generate a batch of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Loss measures generator's ability to fool the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-94694b58c7e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1898\u001b[0m         )\n\u001b[1;32m   1899\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1900\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1901\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set training parameters directly here\n",
        "n_epochs = 200  # number of epochs of training\n",
        "batch_size = 64  # size of the batches\n",
        "lr = 0.0009  # adam: learning rate\n",
        "b1 = 0.5  # adam: decay of first order momentum of gradient\n",
        "b2 = 0.999  # adam: decay of first order momentum of gradient\n",
        "n_cpu = 64  # number of cpu threads to use during batch generation\n",
        "latent_dim = 100  # dimensionality of the latent space\n",
        "img_size = 28  # size of each image dimension\n",
        "channels = 1  # number of image channels\n",
        "sample_interval = 400  # interval between image samples\n",
        "\n",
        "# Image shape (channel, height, width)\n",
        "img_shape = (channels, img_size, img_size)\n",
        "\n",
        "# Check if CUDA is available\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "# Create the Generator class\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "# Define Discriminator class\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity\n",
        "\n",
        "# Loss function\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "\n",
        "# Configure data loader\n",
        "dataloader = DataLoader(\n",
        "    datasets.MNIST(\n",
        "        \"./data/mnist\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose([transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
        "    ),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "# Clear the previous images directory if it exists\n",
        "if os.path.exists('images'):\n",
        "    shutil.rmtree('images')  # This will delete the 'images' directory and all its contents\n",
        "os.makedirs(\"images\", exist_ok=True)  # Recreate the empty directory\n",
        "\n",
        "# Define how often to print and save images (e.g., every 10 batches)\n",
        "print_interval = 100\n",
        "save_interval = 2000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Print the progress every `print_interval` batches\n",
        "        if i % print_interval == 0:\n",
        "            print(f\"[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}]\")\n",
        "\n",
        "        # Save and show images periodically\n",
        "        if i % save_interval == 0:\n",
        "            # Save the generated images\n",
        "            save_image(gen_imgs.data[:25], f\"images/{epoch}_{i}.png\", nrow=5, normalize=True)\n",
        "\n",
        "            # # Display the generated images directly in Colab\n",
        "            # plt.figure(figsize=(5, 5))\n",
        "            # plt.imshow(np.transpose(gen_imgs[0].cpu().detach().numpy(), (1, 2, 0)), cmap='gray')\n",
        "            # plt.axis('off')\n",
        "            # plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ewqc_1REcDT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPe7C9tULbCrz7RQIkYOKNU",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
