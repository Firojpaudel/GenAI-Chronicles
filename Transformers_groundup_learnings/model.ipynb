{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Transformers architecture in detail\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src= \"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"The transformers architecture\" width=\"250px\">\n",
    "    <p style=\"text align: center;\"><i>The Transformers Architecture from the OG paper</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a foundational understanding of the Transformer architecture from the _'Attention is All You Need'_ paper, but now I'm diving deeper into the concepts. Currently, I'm learning from **Umair Jamil** Sir's 'Coding a Transformers from Scratch' course on PyTorch with **Jay Alamar**'s blog combined.\n",
    "\n",
    "Fingers crossed everything goes smoothly! ðŸ¤ž\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the `input_embedding`Layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src= \"https://images.ctfassets.net/k07f0awoib97/2n4uIQh2bAX7fRmx4AGzyY/a1bc6fa1e2d14ff247716b5f589a2099/Screen_Recording_2023-06-03_at_4.52.54_PM.gif\" alt=\"Input Emedding\" width=\"550px\">\n",
    "    <p><i>The Embedding Mechanism illustrated</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##@ All required imports are here in this cell\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \"The embedding is done in pytorch using just a simple nn.Embedding function\"\n",
    "        self.embed= nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * torch.sqrt(self.d_model, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so the `Embedding` was already done earlier but we scale these embeddings while passing forward, for majorly two reasons:\n",
    "\n",
    "1. The `d_model` scaling is done inorder to ensure the magnitude of input embedding and positional encoding are appropriately balanced.\n",
    "2. Also, it maintains the informational integrity.\n",
    "\n",
    "> _src: \"StackOverflow\"_\n",
    "\n",
    "_Well Mathematically_,\n",
    "\n",
    "Let's assume:\n",
    "\n",
    "- Input embeddings as $\\text{E}$\n",
    "- Positional encodings as $\\text{P}$\n",
    "\n",
    "Then, the combined input to the model would be: $$\\text{X} = \\sqrt{d_{model}} \\times \\text{E} + \\text{P}$$\n",
    "\n",
    "\n",
    "Ofc, it will go through the `softmax` function before that but yeah..\n",
    "\n",
    "This scaling ensures that the variance of the embeddings is in line with the variance of the positional encodings, leading to more stable training and better convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `input_embedding`, time for `positional_encoding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_length: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        ## Create a positional encoding matrix of size seq_length x d_model\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        ## Create a vector of shape (seq_length, 1)\n",
    "        pos = torch.arange(0, seq_length).unsqueeze(1)  ## The numerator part \n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model)) ## The denominator part\n",
    "        \n",
    "        ## Apply sine to even positions\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        \n",
    "        ## Apply cosine to odd positions\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        \n",
    "        ## Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0)     \n",
    "        \n",
    "        ## Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explaining the code above:_\n",
    "\n",
    "> \"  Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    " order of the sequence, **we must inject some information about the relative or absolute position of the\n",
    " tokens in the sequence**. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    " bottoms of the encoder and decoder stacks. **The positional encodings have the same dimension dmodel\n",
    " as the embeddings**, so that the two can be summed. There are many choices of positional encodings,\n",
    " learned and fixed \" \n",
    "\n",
    " *-src: paper*\n",
    "\n",
    "Here the authors of this paper have clearly mentioned, first we should inject a sequence and ofc `seq_length` needs to be defined. \n",
    "Next, the dimension we used earlier in the `input_embeddings` is to be used. And, finally there's a `dropout` which is added in order\n",
    "to prevent the model from overfitting.\n",
    "\n",
    "\n",
    "Then again from the same subsection, the `positional_encoding` formulas are provided. We use different formulas for odd and even dimensions.\n",
    "\n",
    "The parameters are same, we just pass through different funcitons. \n",
    "\n",
    "- $sin$ for even dimensions and\n",
    "- $cos$ for odd dimensions.\n",
    "\n",
    "i.e., $$ \\text{PE}_{(pos, 2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$ \n",
    "$$ and, \\text{PE}_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "So, the numerators and denominators are defined before passing through these functions.\n",
    "\n",
    "_Numerator part first:_\n",
    "```python \n",
    "pos = torch.arange(0, seq_len).unsqueeze(1)\n",
    "```\n",
    "\n",
    "Here, the position `pos` is unsquezed to 1, for column vector formatting of shape (seq_length, 1)\n",
    "\n",
    "_Denominator part:_\n",
    "\n",
    "```python\n",
    "div = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)))/ d_model)\n",
    "```\n",
    "\n",
    "Well, it was meant to be $ 10000 ^{\\frac{2i}{d_{model}}}$ but well in coding implementation, we could use log instead.\n",
    "\n",
    "$$ 10000 ^{\\frac{2i}{d_{model}}} \\equiv  exp. (ln(10000) \\cdot \\frac{2i}{d_{model}})$$\n",
    "\n",
    "Also, we are using negative log to ensure the scaling decreases progressively as $i$ increases\n",
    "\n",
    "\n",
    "Next, we add batch dimension to make the positional encoding `pe` compatible with **batch_processing**. The resulting shape is `(1, seq_len, d_model)`. \n",
    "\n",
    "Then, there comes registering buffers... We register the `pe` as a buffer in the `PositionalEncoding` module so that the `pe` is not treated as paprameter but \n",
    "is still part of the state of the module. It's useful for saving and loading models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, entering inside the box; Coding the easiest one of them first.. **Add & Norm** layer aka. _Layer Normalization_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer Normalization**: Technique to stabilize and speed up the training of NNs.\n",
    "\n",
    "Let's assume _a batch_ of 'n' items. Each with mean $\\mu$ and variance $\\sigma ^2 $. \n",
    "\n",
    "Now for normalizing the items in this batch, we calculate the new value as: $$ \\hat{x_{i}} = \\frac{x_{i} - \\mu}{\\sqrt{\\sigma ^2 + \\epsilon}}$$\n",
    "\n",
    "where, $\\epsilon$ is a small constant to prevent the division by zero.\n",
    "\n",
    "Then, there comes scaling and shifting. aka. the multiplicative and additive steps.\n",
    "\n",
    "After normalization, the model applies **learnt parameters** $\\gamma$(scaling) and $\\beta$(shifting) as: $$y_{i} = \\gamma \\cdot \\hat{x_i} + \\beta$$\n",
    "\n",
    "And, this looks so much similar to the the equation of **fully connected layer**. ie., $$y= W \\cdot x + b$$\n",
    "\n",
    "_More on these learnt parameters:_\n",
    "\n",
    "- $\\gamma$ â€” also known as _multiplicative_ or _scaling_ scales (stretch/shrink) the normalized value $\\hat{x_i}$.\n",
    "    - if $\\gamma < 1$, it compresses, \n",
    "    - if $\\gamma > 1$, it amplifies. \n",
    "- $\\beta$ â€” also known as _additive_ or _shifting_ shifts the scaled normalized values up and down. **It allows the network to \"center\" the activations where needed.**\n",
    "\n",
    "> _The $\\gamma$ and $\\beta$ parameters allow flexibility, ensuring the network can adapt the normalized values when necessary._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, epsilon: float = 10**-6):\n",
    "        super.__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = nn.Parameter(torch.ones(1))  # The scaler\n",
    "        self.beta = nn.Parameter(torch.zeros(1))  # The shifter aka bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim= -1, keepdim=True)\n",
    "        std = x.std(dim= -1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Code Explanation incoming:_\n",
    "\n",
    "Okay, so first we had variance in theory part. How come in code, we have standard deviation??\n",
    "\n",
    "Well, standard deviation is just sqrt of variance and the epsilon value, we can just put the square rooted value any way.\n",
    "This way, the coding becomes easier with same functionality.\n",
    "\n",
    "ie., $\\text{standard\\_deviation} = \\sqrt{variance}$ \n",
    "\n",
    "Then, we initialize the value of learnable parameter $\\gamma$ to 1 and $\\beta$ to zero\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next stop, let's expolore the **Feed Forward Layer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper, \n",
    "<div align=\"center\">\n",
    "    <img src= \"./screen_shots_from_the_OG_paper/paper1.jpg\" width=\"600px\">\n",
    "    <p><i>SS from the paper</i></p>\n",
    "</div>\n",
    "\n",
    "First we will calculate the linear transformationâ€”1 by calculating $xW_{1} + b_{1}$ then pass this transformation into $RELU$ activation and then the output acts as input in the next linear transformationâ€”2 as:\n",
    "$$ \\text{FFN}(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # The first linear layer with W1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # The second linear layer with W2 and b2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Description       | Details                                                        |\n",
    "|-------------------|----------------------------------------------------------------|\n",
    "| In Linear Layer 1 | **Input_dimension**: d_model ; **Output_dimension**: d_ff (inner_layer)|\n",
    "| In Linear Layer 2 | **Input_dimension**: d_ff ; **Output_dimension**: back to d_model      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And finally, turn for the most important block; **Multi-Head Attention Block** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gen_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
