{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRJBzngn9mcwF7hI9JIPFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/GenAI-Chronicles/blob/main/BERTs/Seq2Seq_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Learning Seq2Seq Models and trying to implement**\n",
        "---"
      ],
      "metadata": {
        "id": "cNhnAkiXSBld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, what is Seq2Seq Model? Let's define it, then will be focusing on various models related to Seq2Seq."
      ],
      "metadata": {
        "id": "KyzTw1EBd61C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### _Seq2Seq:_"
      ],
      "metadata": {
        "id": "A3W8Cudzh817"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Sequence-to-Sequence (Seq2Seq)** model is a powerful architecture in machine learning designed to transform one sequence into another, making it particularly useful for tasks involving *sequential data*, such as **language translation, text generation, and more**."
      ],
      "metadata": {
        "id": "z3n96TQViDms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Seq2Seq model consists of two main components:\n",
        "___\n",
        "1. **Encoder** \\\n",
        "***Function:*** The encoder processes the input sequence and encodes it into a fixed-size context vector, which represents the essential information of the input data. \\\n",
        "***Architecture:*** Typically implemented using Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, or Gated Recurrent Units (GRUs). The encoder reads the input sequence one element at a time and updates its hidden state accordingly. Once the entire input sequence is processed, the final hidden state is used as the context vector.\n",
        "\n",
        "2. **Decoder** \\\n",
        "***Function:*** The decoder takes the context vector produced by the encoder and generates the output sequence step-by-step. \\\n",
        "***Process:*** The decoder operates in an autoregressive manner, meaning it generates one element of the output at each time step while considering its previous outputs. It uses both the context vector and its own previous hidden states to predict the next element in the sequence.\n",
        "\n",
        "---\n",
        "<figure align= \"center\">\n",
        "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Ismhi-muID5ooWf3ZIQFFg.png\" alt=\"Encoder_Decoder_s2s_illustrating_architecture\" width= \"550\"/>\n",
        "  <figcaption><i>Simple illustration of the process</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XBYBnYFKipCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXHLXiKmjWl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}