{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Firojpaudel/GenAI-Chronicles/blob/main/BERTs/BERT_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQPukOnHkuE1"
      },
      "source": [
        "## **BERT**: _Using Huggingface_ ðŸ¤—\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK6j5lDknrr2"
      },
      "source": [
        "I'm learning from [The Official Hugging Face Transformer Docs](https://huggingface.co/docs/transformers/index). And I'll be using PyTorch the entire time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YK_YPdulGUG"
      },
      "outputs": [],
      "source": [
        "##@ First lets install the huggingface trannsformers, datasets, evaluate and accelerate\n",
        "! pip install transformers datasets evaluate accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Ek3e1hXtzZSL"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "my_token = userdata.get('HF_collab')  #Loading the Hugging Face Access Token through the secretKey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "q_WSQkZ11EAk"
      },
      "outputs": [],
      "source": [
        "## Then Logging in:\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(my_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_XcMGTblcF3"
      },
      "source": [
        "### Getting Started:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqI3Zf4wpvaP"
      },
      "source": [
        "#### **A. Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtqUa9QNSD4-"
      },
      "source": [
        "**Important Catalogue before starting:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| **Task**                     | **Description**                                                                                              | **Modality**    | **Pipeline identifier**                       |\n",
        "|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|\n",
        "| Text classification          | assign a label to a given sequence of text                                                                   | NLP             | pipeline(task=â€œsentiment-analysisâ€)           |\n",
        "| Text generation              | generate text given a prompt                                                                                 | NLP             | pipeline(task=â€œtext-generationâ€)              |\n",
        "| Summarization                | generate a summary of a sequence of text or document                                                         | NLP             | pipeline(task=â€œsummarizationâ€)                |\n",
        "| Image classification         | assign a label to an image                                                                                   | Computer vision | pipeline(task=â€œimage-classificationâ€)         |\n",
        "| Image segmentation           | assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation) | Computer vision | pipeline(task=â€œimage-segmentationâ€)           |\n",
        "| Object detection             | predict the bounding boxes and classes of objects in an image                                                | Computer vision | pipeline(task=â€œobject-detectionâ€)             |\n",
        "| Audio classification         | assign a label to some audio data                                                                            | Audio           | pipeline(task=â€œaudio-classificationâ€)         |\n",
        "| Automatic speech recognition | transcribe speech into text                                                                                  | Audio           | pipeline(task=â€œautomatic-speech-recognitionâ€) |\n",
        "| Visual question answering    | answer a question about the image, given an image and a question                                             | Multimodal      | pipeline(task=â€œvqaâ€)                          |\n",
        "| Document question answering  | answer a question about a document, given an image and a question                                            | Multimodal      | pipeline(task=\"document-question-answering\")  |\n",
        "| Image captioning             | generate a caption for a given image                                                                         | Multimodal      | pipeline(task=\"image-to-text\")                |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aCAAzLcpz9V",
        "outputId": "539965cc-7645-4936-b652-0cda022b7f3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier= pipeline('sentiment-analysis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B25koaclRAe2",
        "outputId": "dd029204-ec34-4e62-9b14-1752d9ce9289"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9996222257614136}]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier(\"Hey! so this way we can pass the value to classifier and its super easy. I'm liking this!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lwLWSPqVh-2",
        "outputId": "d6c4c95d-646c-42e7-babc-9c59d8485a21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9874186515808105}]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@ Testing for negative sentiment\n",
        "classifier(\"You dummy!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c51tqs6V579"
      },
      "source": [
        "Likewise, if we have more than one inputs, we can pass inputs as lists to the `pileline()` and that will return the list of dictionaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWWg3iTWWNC2",
        "outputId": "965f3b71-78fd-4b5d-fa3e-2ad0849b722b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label: POSITIVE, score: 0.9998769760131836\n",
            "label: NEGATIVE, score: 0.9993403553962708\n"
          ]
        }
      ],
      "source": [
        "results = classifier([\"You look beautiful\", \"You ugly hag!\"])\n",
        "for result in results:\n",
        "  print(f\"label: {result['label']}, score: {result['score']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkovCoZvW1Ft"
      },
      "source": [
        "Also the `pipeline()` can iterate over the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cwvoOn7Yg9t",
        "outputId": "58fc1f28-2955-4e90-a49f-e0fc6bf1473c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "speech_recognizer = pipeline(\"automatic-speech-recognition\", model= \"facebook/wav2vec2-base-960h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iD3MGrHOZFPW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Audio\n",
        "\n",
        "dataset= load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCQY1dccgca_"
      },
      "source": [
        "What we are dong in above code snippet is that: we are loading the specific \"PolyAIs MINDS-14\" dataset from the Huggingface hub.\n",
        "\n",
        "Likewise `en-us` as name specifies the particular subset or config of the dataset. In this case, it loads the English(US) subset of the dataset.\n",
        "\n",
        "\n",
        "Then there comes `split=\"train\"` which specifies the split of the dataset to load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Ks5EDIY9ZsTX"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHMErBwNZ4_G"
      },
      "source": [
        "More explanations:\n",
        "\n",
        "1. The `cast_column` Method:\n",
        "Its similar to type_casting that we used to do in basics of python. Here, what it does is, it modifies the \"audio\" column to use the `Audio` type with a specified sampling rate.\n",
        "\n",
        "_Why cast_column?_\n",
        "\n",
        "- So that all the data are standardized  to the desired format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na8qsWFFZ9tG",
        "outputId": "8045564f-b172-448b-b73b-9821781a3076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE\"]\n"
          ]
        }
      ],
      "source": [
        "result = speech_recognizer(dataset[:2][\"audio\"])\n",
        "print([d[\"text\"] for d in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFLNGqLqaIYN"
      },
      "source": [
        "> **Note**: \\\n",
        "Incase of the larger datasets like Audio and Images, we can use `generators` to avoid the memory overload. \\\n",
        "And, the HG-pipeline API can work seamlessly with these  geneators for effecient processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hlx6cNTqpdq"
      },
      "source": [
        "#### Using another model and tokenizer in the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RiGu53Jq48y"
      },
      "source": [
        "Well, before when we used pipeline, we didn't mention the `model` during the _\"sentiment-analysis\"_ and by default it used: `distilbert/distilbert-base-uncased-finetuned-sst-2-english` model which just classifies the english text.\n",
        "\n",
        "Now, lets try calling the model which also works with French, Spanish languages.\n",
        "\n",
        "We will be using [this model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3jdHMBc7rq8q"
      },
      "outputs": [],
      "source": [
        "model_name  = \"nlptown/bert-base-multilingual-uncased-sentiment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQdvtzu7sUu0",
        "outputId": "56a73017-818a-4996-8361-5208eae6eb39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model= model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PRndRYishzk",
        "outputId": "ba1e88b0-a606-4e52-ba29-2b69956185a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label: 5 stars, score: 0.3779810667037964\n",
            "label: 1 star, score: 0.7339279651641846\n"
          ]
        }
      ],
      "source": [
        "##@ Lets try with Dutch sentences one with positive and another with negative sentiment\n",
        "tests_dutch= classifier([\"hey daar jochie, hoe gaat het? je ziet er onstuimig uit!!\", \"Hoe kan iemand er zo slecht uitzien?\"])\n",
        "for test_dutch in tests_dutch:\n",
        "  print(f\"label: {test_dutch['label']}, score: {test_dutch['score']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5fylGdEs5-L"
      },
      "source": [
        "#### **B. AutoClass**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0k2pfEE5QYC"
      },
      "source": [
        "This feature provides an abstraction that automatically selects and loads the appropriate model and tokenizer for a given task based on model's architecture.\n",
        "\n",
        "> _This allows us to work with the pretrained models without needing to know the exact details of their configuration or class implementation._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RO0IqNY9tEq"
      },
      "source": [
        "##### B.1 AutoTokenizer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NM-A8d2H92Gq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2iCBRG0-FPr",
        "outputId": "67f0b19e-10af-4f0b-9b20-61febaeab0cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 32821, 35070, 106, 11523, 112, 161, 10700, 106, 10855, 12050, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding= tokenizer(\"Hey bud! What's up! You good?\")\n",
        "encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drbwLSUw-QB6"
      },
      "source": [
        "What might have happened internally:\n",
        "\n",
        "---\n",
        "1. **Text Normalization:**\\\n",
        "The input is cleaned (lowercased if using an uncased model, etc.).\n",
        "\n",
        "2. **Tokenization:**\\\n",
        "The text is split into tokens (words, subwords, or special symbols) using the tokenizer's predefined rules. \\\n",
        "*Example:* `\"Hey bud! What's up! You good?\"` might be split into:\\\n",
        "`[\"hey\", \"bud\", \"!\", \"what\", \"'\", \"s\", \"up\", \"!\", \"you\", \"good\", \"?\"]`.\n",
        "\n",
        "3. **Mapping to IDs:**\\\n",
        "Each token is mapped to its corresponding ID in the tokenizer's vocabulary. For example:\\\n",
        "`\"hey\" â†’ 32821`\n",
        "`\"!\" â†’ 106`\n",
        "\n",
        "4. **Adding Special Tokens:**\\\n",
        "Special tokens like `[CLS]` (start of sequence) and `[SEP]` (end of sequence) are added.\n",
        "Example: `[CLS] hey bud ! what ' s up ! you good ? [SEP]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB8lHrvME0xl"
      },
      "source": [
        "There's a better format we could use. Which is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67DPrt0EAvHy",
        "outputId": "dfa3260f-ef64-4e85-b248-44d5df0d1837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103,   100,\n",
            "         58263, 13299,   119,   102],\n",
            "        [  101, 11312, 18763, 10855, 11530,   112,   162, 39487, 10197,   119,\n",
            "           102,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        " #@ The example code snippet from the docs itself\n",
        " pt_batch = tokenizer(\n",
        "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "print(pt_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WydG6AdqB7ZD"
      },
      "source": [
        "Why this format?\n",
        "- This format ensures all sequences in the batch are of equal length _(a requirement for transformer models)_. The `attention_mask` tells the model which parts of the input are meaningful and which are just padding.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBdyWkEoFSqk"
      },
      "source": [
        "##### B.2. AutoModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqtQv5kPFtCT"
      },
      "source": [
        "- `AutoModel` is a generic class used to automatically load a pretrained transformer model.\n",
        "\n",
        "- Itâ€™s designed to reduce the need for users to manually specify the model architecture (like BERT, RoBERTa, GPT, etc.).\n",
        "\n",
        "- By specifying the name or path of a pretrained model, AutoModel figures out the right architecture for the task.\n",
        "\n",
        "- For different tasks, there are specific variants of AutoModel:\n",
        "\n",
        "  - **AutoModelForSequenceClassification**: For text classification tasks.\n",
        "  - **AutoModelForTokenClassification**: For tasks like Named Entity Recognition (NER).\n",
        "  - **AutoModelForQuestionAnswering**: For question-answering tasks.\n",
        "  -**AutoModelForCausalLM**: For text generation tasks. etc.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOdPKNMZIbxt",
        "outputId": "44722c59-6c37-4fc4-85ae-7bf6a0172790"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification as AmFSC\n",
        "\n",
        "pt_model = AmFSC.from_pretrained(model_name)\n",
        "pt_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIg2WdnrJHa7"
      },
      "source": [
        "So it just recognized it as a BERT based model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOB8LgKHJP-U",
        "outputId": "feb1ff5d-300d-49d4-aea3-0297f45f1352"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-2.6222, -2.7745, -0.8967,  2.0137,  3.3064],\n",
              "        [ 0.0064, -0.1258, -0.0503, -0.1655,  0.1329]],\n",
              "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pt_outputs= pt_model(**pt_batch)\n",
        "pt_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eP95sHNJgND"
      },
      "source": [
        "The `pt_model(**pt_batch)` line performs a forward pass through the model. The model computes the logits _(raw, un-normalized scores for each class)_ for the input text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFIcoof2LQuk"
      },
      "source": [
        "Now, if we want to view the probablities using these logits values, we can."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T8VGxqgLf10",
        "outputId": "33c1c913-c886-428e-c0b3-cfa854d5829d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
              "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "pt_pred = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
        "pt_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djveXz3zLrez"
      },
      "source": [
        "Now, why `dim= -1` ?\n",
        "\n",
        "---\n",
        "\n",
        "- `dim= -/+ 1 ` calculates the probabilities along last dimension _(across columns for each row)_ ie. PyTorch would normalize row-wise.\n",
        "\n",
        "- `dim = 0` would normalize column-wise instead of row-wise.\n",
        "  - This is wrong for classification tasks because:\n",
        "    - Youâ€™d mix up scores from different examples, which doesnâ€™t make sense.\n",
        "\n",
        "> _**Note:**_  \n",
        "- `dim=-1` is universal, flexible, and works for any tensor shape where the last dimension is the target.\n",
        "- `dim=1` is fine in specific cases, but it's less robust and not future-proof if your tensor shapes change.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8mxiRqpOgYr"
      },
      "source": [
        "#### **C. Saving the Model**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdK20r6Tw27E"
      },
      "source": [
        "After finetuning the model, we can save it with its tokenizer using `{pretrainedmodel}.save_pretrained()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "g0S9JrX00Pfe"
      },
      "outputs": [],
      "source": [
        "pt_save_directory = \"./pt_save_pretrained\"\n",
        "tokenizer.save_pretrained(pt_save_directory)\n",
        "pt_model.save_pretrained(pt_save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qLTxocp0RMC"
      },
      "source": [
        "When we want to use model again, just use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "i5j6SPuS0XiK"
      },
      "outputs": [],
      "source": [
        "pt_model = AmFSC.from_pretrained(\"./pt_save_pretrained\")  #I imported AutoModelForSeqClassification as AmFSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPI7ejw70kfK"
      },
      "source": [
        "**The Cool Feture in ðŸ¤— transformers:**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1XvTwr61Gda"
      },
      "source": [
        "The saved model could be reused as both TF or PyTorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nA9G0Fb1M4B",
        "outputId": "03e33a60-8cbe-4bd1-dbe9-3589d799d6b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification as TFAmFSC ##Note: TensorFlow has this instead :)\n",
        "#@ If we want to convert this pt model to tf model,\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
        "tf_model = TFAmFSC.from_pretrained(pt_save_directory, from_pt= True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAwMk_WOToVw"
      },
      "source": [
        "#### **D. Building the Custom Models**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5g_hPXzHTuYp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "my_config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", n_heads=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3w3f59cUPSY",
        "outputId": "75dca66a-e6c9-4653-ceb4-3b555ee740b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(my_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "0fdppcXnU1Pp"
      },
      "outputs": [],
      "source": [
        "##@ Using this custom configuration we could create a new model as well\n",
        "from transformers import AutoModel\n",
        "model_custom = AutoModel.from_config(my_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_Em5VNEVM44"
      },
      "source": [
        "> _And we could perform rem tasks as before on this model created.._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r33IluIXVpzQ"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
