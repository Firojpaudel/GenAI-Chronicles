{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Firojpaudel/GenAI-Chronicles/blob/main/Transformers_groundup_learnings/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> \n",
    "[![Run in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/kernels/welcome?src=https://github.com/Firojpaudel/GenAI-Chronicles/blob/main/Transformers_groundup_learnings/model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Transformers architecture in detail\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src= \"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"The transformers architecture\" width=\"250px\">\n",
    "    <p><i>The Transformers Architecture from the OG paper</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a foundational understanding of the Transformer architecture from the _'Attention is All You Need'_ paper, but now I'm diving deeper into the concepts. Currently, I'm learning from **Umair Jamil** Sir's 'Coding a Transformers from Scratch' course on PyTorch with **Jay Alamar**'s blog combined.\n",
    "\n",
    "Fingers crossed everything goes smoothly! ðŸ¤ž\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the `input_embedding`Layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src= \"https://images.ctfassets.net/k07f0awoib97/2n4uIQh2bAX7fRmx4AGzyY/a1bc6fa1e2d14ff247716b5f589a2099/Screen_Recording_2023-06-03_at_4.52.54_PM.gif\" alt=\"Input Emedding\" width=\"550px\">\n",
    "    <p><i>The Embedding Mechanism illustrated</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##@ All required imports are here in this cell\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \"The embedding is done in pytorch using just a simple nn.Embedding function\"\n",
    "        self.embed= nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * torch.sqrt(self.d_model, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so the `Embedding` was already done earlier but we scale these embeddings while passing forward, for majorly two reasons:\n",
    "\n",
    "1. The `d_model` scaling is done inorder to ensure the magnitude of input embedding and positional encoding are appropriately balanced.\n",
    "2. Also, it maintains the informational integrity.\n",
    "\n",
    "> _src: \"StackOverflow\"_\n",
    "\n",
    "_Well Mathematically_,\n",
    "\n",
    "Let's assume:\n",
    "\n",
    "- Input embeddings as $\\text{E}$\n",
    "- Positional encodings as $\\text{P}$\n",
    "\n",
    "Then, the combined input to the model would be: $$\\text{X} = \\sqrt{d_{model}} \\times \\text{E} + \\text{P}$$\n",
    "\n",
    "\n",
    "Ofc, it will go through the `softmax` function before that but yeah..\n",
    "\n",
    "This scaling ensures that the variance of the embeddings is in line with the variance of the positional encodings, leading to more stable training and better convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `input_embedding`, time for `positional_encoding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_length: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        ## Create a positional encoding matrix of size seq_length x d_model\n",
    "        pe = torch.zeros(seq_length, d_model)\n",
    "        ## Create a vector of shape (seq_length, 1)\n",
    "        pos = torch.arange(0, seq_length).unsqueeze(1)  ## The numerator part \n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model)) ## The denominator part\n",
    "        \n",
    "        ## Apply sine to even positions\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        \n",
    "        ## Apply cosine to odd positions\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        \n",
    "        ## Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0)     \n",
    "        \n",
    "        ## Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Explaining the code above:_\n",
    "\n",
    "> \"  Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    " order of the sequence, **we must inject some information about the relative or absolute position of the\n",
    " tokens in the sequence**. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    " bottoms of the encoder and decoder stacks. **The positional encodings have the same dimension dmodel\n",
    " as the embeddings**, so that the two can be summed. There are many choices of positional encodings,\n",
    " learned and fixed \" \n",
    "\n",
    " *-src: paper*\n",
    "\n",
    "Here the authors of this paper have clearly mentioned, first we should inject a sequence and ofc `seq_length` needs to be defined. \n",
    "Next, the dimension we used earlier in the `input_embeddings` is to be used. And, finally there's a `dropout` which is added in order\n",
    "to prevent the model from overfitting.\n",
    "\n",
    "\n",
    "Then again from the same subsection, the `positional_encoding` formulas are provided. We use different formulas for odd and even dimensions.\n",
    "\n",
    "The parameters are same, we just pass through different funcitons. \n",
    "\n",
    "- $sin$ for even dimensions and\n",
    "- $cos$ for odd dimensions.\n",
    "\n",
    "i.e., $$ \\text{PE}_{(pos, 2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$ \n",
    "$$ and, \\text{PE}_{(pos, 2i+1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "So, the numerators and denominators are defined before passing through these functions.\n",
    "\n",
    "_Numerator part first:_\n",
    "```python \n",
    "pos = torch.arange(0, seq_len).unsqueeze(1)\n",
    "```\n",
    "\n",
    "Here, the position `pos` is unsquezed to 1, for column vector formatting of shape (seq_length, 1)\n",
    "\n",
    "_Denominator part:_\n",
    "\n",
    "```python\n",
    "div = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)))/ d_model)\n",
    "```\n",
    "\n",
    "Well, it was meant to be $ 10000 ^{\\frac{2i}{d_{model}}}$ but well in coding implementation, we could use log instead.\n",
    "\n",
    "$$ 10000 ^{\\frac{2i}{d_{model}}} \\equiv  exp. (ln(10000) \\cdot \\frac{2i}{d_{model}})$$\n",
    "\n",
    "Also, we are using negative log to ensure the scaling decreases progressively as $i$ increases\n",
    "\n",
    "\n",
    "Next, we add batch dimension to make the positional encoding `pe` compatible with **batch_processing**. The resulting shape is `(1, seq_len, d_model)`. \n",
    "\n",
    "Then, there comes registering buffers... We register the `pe` as a buffer in the `PositionalEncoding` module so that the `pe` is not treated as paprameter but \n",
    "is still part of the state of the module. It's useful for saving and loading models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, entering inside the box; Coding the easiest one of them first.. **Add & Norm** layer aka. _Layer Normalization_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer Normalization**: Technique to stabilize and speed up the training of NNs.\n",
    "\n",
    "Let's assume _a batch_ of 'n' items. Each with mean $\\mu$ and variance $\\sigma ^2 $. \n",
    "\n",
    "Now for normalizing the items in this batch, we calculate the new value as: $$ \\hat{x_{i}} = \\frac{x_{i} - \\mu}{\\sqrt{\\sigma ^2 + \\epsilon}}$$\n",
    "\n",
    "where, $\\epsilon$ is a small constant to prevent the division by zero.\n",
    "\n",
    "Then, there comes scaling and shifting. aka. the multiplicative and additive steps.\n",
    "\n",
    "After normalization, the model applies **learnt parameters** $\\gamma$(scaling) and $\\beta$(shifting) as: $$y_{i} = \\gamma \\cdot \\hat{x_i} + \\beta$$\n",
    "\n",
    "And, this looks so much similar to the the equation of **fully connected layer**. ie., $$y= W \\cdot x + b$$\n",
    "\n",
    "_More on these learnt parameters:_\n",
    "\n",
    "- $\\gamma$ â€” also known as _multiplicative_ or _scaling_ scales (stretch/shrink) the normalized value $\\hat{x_i}$.\n",
    "    - if $\\gamma < 1$, it compresses, \n",
    "    - if $\\gamma > 1$, it amplifies. \n",
    "- $\\beta$ â€” also known as _additive_ or _shifting_ shifts the scaled normalized values up and down. **It allows the network to \"center\" the activations where needed.**\n",
    "\n",
    "> _The $\\gamma$ and $\\beta$ parameters allow flexibility, ensuring the network can adapt the normalized values when necessary._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, epsilon: float = 10**-6):\n",
    "        super.__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = nn.Parameter(torch.ones(1))  # The scaler\n",
    "        self.beta = nn.Parameter(torch.zeros(1))  # The shifter aka bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim= -1, keepdim=True)\n",
    "        std = x.std(dim= -1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Code Explanation incoming:_\n",
    "\n",
    "Okay, so first we had variance in theory part. How come in code, we have standard deviation??\n",
    "\n",
    "Well, standard deviation is just sqrt of variance and the epsilon value, we can just put the square rooted value any way.\n",
    "This way, the coding becomes easier with same functionality.\n",
    "\n",
    "ie., $\\text{standard\\_deviation} = \\sqrt{variance}$ \n",
    "\n",
    "Then, we initialize the value of learnable parameter $\\gamma$ to 1 and $\\beta$ to zero\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next stop, let's expolore the **Feed Forward Layer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper, \n",
    "<div align=\"center\">\n",
    "    <img src= \"./screen_shots_from_the_OG_paper/Feed_forward.jpg\" width=\"600px\">\n",
    "    <p><i>SS from the paper</i></p>\n",
    "</div>\n",
    "\n",
    "First we will calculate the linear transformationâ€”1 by calculating $xW_{1} + b_{1}$ then pass this transformation into $RELU$ activation and then the output acts as input in the next linear transformationâ€”2 as:\n",
    "$$ \\text{FFN}(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # The first linear layer with W1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # The second linear layer with W2 and b2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Description       | Details                                                        |\n",
    "|-------------------|----------------------------------------------------------------|\n",
    "| In Linear Layer 1 | **Input_dimension**: d_model ; **Output_dimension**: d_ff (inner_layer)|\n",
    "| In Linear Layer 2 | **Input_dimension**: d_ff ; **Output_dimension**: back to d_model      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And finally, turn for the most important block; **Multi-Head Attention Block** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the Multi-Head Attention Block, we need to know about the Scaled Dot-product attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaled Dot-Product Mechanism**: \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./screen_shots_from_the_OG_paper/Scaled-dot_product_attn.jpg\" width=\"250px\">\n",
    "    <p><i>Figure representing Scaled Dot-product Attention</i></p>\n",
    "</div>\n",
    "\n",
    "_Explaining the process:_\n",
    "\n",
    "1. **Sentence Breakdown**:\n",
    "    - Start with any sentence. This sentence is broken down into \"Query\", \"Keys\", and \"Values\".\n",
    "\n",
    "2. **Query-Key Multiplication**:\n",
    "    - Multiply the Query and Keys. This multiplication helps in determining the relevance of each key with respect to the query.\n",
    "\n",
    "3. **Scaling**:\n",
    "    - Scale the multiplied values. This step ensures that the values are within a manageable range, preventing extremely large values that could destabilize the training process.\n",
    "\n",
    "4. **Masking (Optional)**:\n",
    "    - Pass the scaled values through a masking layer. Masking is used to prevent attending to certain positions, typically in the context of sequence-to-sequence models.\n",
    "\n",
    "5. **Activation Function**:\n",
    "    - Apply the activation function (softmax, as mentioned in the paper). This step converts the scaled values into probabilities, highlighting the most relevant keys.\n",
    "\n",
    "        - Given by formula: $$\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_{k}}})V $$\n",
    "\n",
    "        where,\n",
    "\n",
    "        - $d_{k}$ â€” dimension of keys == dimension of queries (so they can be multiplied duhh.. ) &&\n",
    "\n",
    "        - $d_{v}$ â€” dimension of values.\n",
    "\n",
    "6. **Final Multiplication**:\n",
    "    - Multiply the output of the activation function with the Values. This step produces the `Attention(Q, K, V)` value, which is a weighted sum of the values based on the relevance scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Head Attention**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./screen_shots_from_the_OG_paper/Multihead.jpg\" width=\"250px\">\n",
    "    <p><i>Figure Representing the Multi-Head Attention Process</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Scaled Dot-Product Attention, we focus on a single set of queries, keys, and values. However, with Multi-Head Attention, we apply this mechanism across multiple sets of queries, keys, and values, enabling the model to capture different features from the input sequence.\n",
    "\n",
    "- Each set of queries, keys, and values is linearly projected before being fed into the Scaled Dot-Product Attention layer. This allows each head to focus on different parts of the input, enhancing the model's ability to understand complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model= d_model\n",
    "        self.h= h\n",
    "        assert d_model % h ==0, \"d_model is not divisible by h\"\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        attention_scores = query @ key.transpose(-2, -1) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim = -1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "            \n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        \n",
    "        query = query.view(query.size[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.size[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.size[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        ## (Batch_size, h, seq_length, d_k) -> (Batch_size, seq_length, h, d_k) -> (Batch_size, seq_length, d_model)\n",
    "        x= x.transpose(1, 2).contiguous().view(x.size(0), -1, self.h * self.d_k)  #@ self.h * self.d_k = d_model\n",
    "        \n",
    "        #@ Now finally we multiply this with the output weights:\n",
    "        return self.w_o(x)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Code Explanation of above Cell:_\n",
    "\n",
    "- **Starting from `h`**: `h` is the number of attention heads. The assertion ensures `d_model` is divisible by `h` so that each head gets an equal portion of model dimension.\n",
    "- `d_k`: The dimension of each attention head.\n",
    "- `w_q, w_k, w_v`: Are the linear layers to project input queries(Q), keys(K) and values(V) from `d_model` to `d_model`.\n",
    "- `w_o`: is the linear layer to combine the outputs of attention heads\n",
    "\n",
    "- Next, we have defined the attention function statically, \n",
    "\n",
    "    Reason behind defining with `@staticmethod`:\n",
    "    - **No Dependency on Class Instance**: The attention function doesn't need any instance-specific data or attributes to perform its operations. It relies solely on the inputs provided (`query`, `key`, `value`, `mask`, and `dropout`). Therefore, it can be defined as a static method because it doesn't require access to the instance (self) of the class.\n",
    "    - **Reusability**: Static methods can be called without needing to create an instance of the class. This makes the attention function more flexible and reusable in different contexts, even outside the `MultiHeadAttention` class if needed.\n",
    "\n",
    "- Weâ€™ve reintroduced the dimension d_k inside the static function because the static function acts independently. This time, we simply extract the last value from the query shape as d_k, which represents the dimension of the embedded vectors. This works because itâ€™s just a matrix multiplication after all. ðŸ¤” _(The figure at the end of this cell will definitely help us understand that)_ \n",
    "\n",
    "- Then, in `atttention_scores`, we have just applied the formula $ \\frac{Q K^T}{\\sqrt{d_{k}}}$. Also, transposing the second last and last positions.\n",
    "\n",
    "> PS: If you are wondering what @ is, learn python haiyaa.. why are you even here? âœ‹ðŸ˜•ðŸ¤š\n",
    "\n",
    "- Likewise in the same `attention_score`, if there is mask provided, masking very large negative values to 0, just ignoring...\n",
    "\n",
    "- Then, pass it through the softmax activation\n",
    "\n",
    "- Also,if the dropout exists, apply it in `attention_scores`. It would prevent overfitting.\n",
    "\n",
    "- Then matrix multiplication with values and attention score itself.\n",
    "\n",
    "_In formard function:_\n",
    "\n",
    "- We just passed query,key and value through the respective linear layers.\n",
    "\n",
    "Now explaining this section:\n",
    "```python \n",
    "query = query.view(query.size[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "key = key.view(key.size[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "value = value.view(value.size[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "```\n",
    "\n",
    "Okay so typically, the shape/size of `q,k and v` are in the format: `(Batch_size, seq_len, d_model)`. Now, we are just changing the view to be in the format of : `(Batch_size, seq_len, h, d_k)`\n",
    "\n",
    "Then after transposing 1 and 2 positions, we get the shape of: `(Batch_size, h, seq_len, d_k)`\n",
    "\n",
    "But the major question is why change the view?\n",
    "Answer to that is here:\n",
    "\n",
    "1. **Multi-Head Attention Mechanism:** In the Transformer model, the attention mechanism is split into multiple heads. This allows the model to focus on different parts of the input sequence simultaneously. Each head has its own set of weights and performs its own attention calculation.\n",
    "\n",
    "2. **Reshaping for Parallel Computation:** The original shape of the tensors `query, key, and value` is `(Batch_size, seq_len, d_model)`. `d_model` is the dimension of the model, which is split into `h` heads, each with a dimension of `d_k` (where $d_{k} = \\frac{d_{model}}{h}$). By reshaping the tensors to `(Batch_size, seq_len, h, d_k)`, we prepare them for parallel computation across the multiple heads.\n",
    "\n",
    "3. **Transposing for Efficient Computation:** After reshaping, we transpose the tensors to `(Batch_size, h, seq_len, d_k)`.\n",
    "This transposition ensures that the heads are the second dimension, which allows for efficient computation of attention scores across the heads.\n",
    "\n",
    "Then lets talk about  the contiguous memory allocation. The `.contiguous()` function ensures that the tensor's memory layout is suitable for the next operation `(view)`. This step is necessary because transpose can result in non-contiguous memory storage, which is incompatible with reshaping.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./screen_shots_from_the_OG_paper/Mechanism_explained.png\">\n",
    "    <p><i>Multihead Attention visually explained</i></p>\n",
    "    <p><i>*Courtesy: Umair Jamil sirs video*</i></p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, now finally, we define the **residual_connections** or the **skip_connections**, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the required functions for the Encoder/Decoder blocks are done. Now, we just need to define the Encoder block...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock (nn.Module):\n",
    "    def __init__(self, self_attn: MultiHeadAttention, feed_forward: FeedForward, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "        \n",
    "    def forward(self,x, mask): \n",
    "        ''' why apply mask here?:\n",
    "        The mask is applied to the self attention mechanism to prevent the model from attending to the future tokens.'''\n",
    "        x= self.residual_connection[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        x= self.residual_connection[1](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Code Explanations:_\n",
    "\n",
    "So I was gonna use \n",
    "```python \n",
    "    self.residual_connection_1 = ResidualConnection(dropout)\n",
    "    self.residual_connection_2 = ResidualConnection(dropout)\n",
    "```\n",
    "but well found out that using ModuleList is more appropriate for this kind of things...\n",
    "\n",
    "Now, I'd like to explain the forward function in this module:\n",
    "```python\n",
    "x= self.residual_connection[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "x= self.residual_connection[1](x, self.feed_forward)\n",
    "```\n",
    "So here, first lets bring in the figure to explain this more clearly.\n",
    "\n",
    "<div align= \"center\">\n",
    "    <img src= \"./screen_shots_from_the_OG_paper/Encoder_Block_zoomed.jpg\" width= \"300|px\">\n",
    "    <p><i>Encoder Block Zoomed with labelled Skip/Residual Connections</i></p>\n",
    "</div>\n",
    "\n",
    "The first residual or skip connection is defined as `[0]`, and the lambda function is used to pass arguments to the self_attn function dynamically. Specifically, it allows the self-attention layer to take $Q,K,V$ `(query, key, value)` all as $x$, along with the mask.\n",
    "\n",
    "The second residual connection, `[1]`, wraps the feed-forward block, applying it directly to the output from the first residual connection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defining the Encoder as a whole:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x= layer(x, mask)\n",
    "        return self.norm(x)  #Since after every layer we have Add and Norm layer.. ðŸ¤·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this encoder layer is pretty much done...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets go for the **decoder block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, self_attn: MultiHeadAttention, cross_attn: MultiHeadAttention, \\\n",
    "                 feed_forward: FeedForward, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.cross_attn = cross_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, trgt_mask):\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attn(x, x, x, trgt_mask))\n",
    "        x = self.residual_connection[1](x, lambda x: self.cross_attn(x, enc_output, enc_output, src_mask))\n",
    "        x = self.residual_connection[2](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Now coding the Decoder class '''\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask, trgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, trgt_mask)\n",
    "        return self.norm(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gen_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
