{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Transformers architecture in detail\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src= \"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"The transformers architecture\" width=\"250px\">\n",
    "    <figcaption><i>The Transformers Architecture from the OG paper</i></figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a foundational understanding of the Transformer architecture from the _'Attention is All You Need'_ paper, but now I'm diving deeper into the concepts. Currently, I'm learning from **Umair Jamil** Sir's 'Coding a Transformers from Scratch' course on PyTorch with **Jay Alamar**'s blog combined.\n",
    "\n",
    "Fingers crossed everything goes smoothly! ðŸ¤ž\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the `input_embedding`Layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src= \"https://images.ctfassets.net/k07f0awoib97/2n4uIQh2bAX7fRmx4AGzyY/a1bc6fa1e2d14ff247716b5f589a2099/Screen_Recording_2023-06-03_at_4.52.54_PM.gif\" alt=\"Input Emedding\" width=\"550px\">\n",
    "    <figcaption><i>The Embedding Mechanism illustrated</i></figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##@ All required imports are here in this cell\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \"The embedding is done in pytorch using just a simple nn.Embedding function\"\n",
    "        self.embed= nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * torch.sqrt(self.d_model, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so the `Embedding` was already done earlier but we scale these embeddings while passing forward, for majorly two reasons:\n",
    "\n",
    "1. The `d_model` scaling is done inorder to ensure the magnitude of input embedding and positional encoding are appropriately balanced.\n",
    "2. Also, it maintains the informational integrity.\n",
    "\n",
    "> _src: \"StackOverflow\"_\n",
    "\n",
    "_Well Mathematically_,\n",
    "\n",
    "Let's assume:\n",
    "\n",
    "- Input embeddings as $\\text{E}$\n",
    "- Positional encodings as $\\text{P}$\n",
    "\n",
    "Then, the combined input to the model would be: $$\\text{X} = \\sqrt{d_{model}} \\times \\text{E} + \\text{P}$$\n",
    "\n",
    "\n",
    "Ofc, it will go through the `softmax` function before that but yeah..\n",
    "\n",
    "This scaling ensures that the variance of the embeddings is in line with the variance of the positional encodings, leading to more stable training and better convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `input_embedding`, time for `positional_encoding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_length: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_length = seq_length\n",
    "        self.dropout = nn.Dropout(dropout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gen_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
